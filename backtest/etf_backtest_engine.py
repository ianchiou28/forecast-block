"""
ETFé¢„æµ‹ç³»ç»Ÿ - ETFæ»šåŠ¨å›æµ‹å¼•æ“
çœŸæ­£çš„å†å²å›æµ‹éªŒè¯ï¼šç”¨è¿‡å»æ•°æ®è®­ç»ƒï¼Œé¢„æµ‹æœªæ¥æ•°æ®
æ”¯æŒ3å¹´å†å²æ•°æ®è®­ç»ƒ
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Tuple
import logging
from pathlib import Path
import pickle
import json
import warnings
import time

import akshare as ak
import lightgbm as lgb
from sklearn.metrics import ndcg_score, mean_squared_error

warnings.filterwarnings('ignore')

PROJECT_ROOT = Path(__file__).parent.parent
BACKTEST_RESULTS_DIR = PROJECT_ROOT / "data" / "backtest_results"
BACKTEST_RESULTS_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger(__name__)


# å®Œæ•´ETFæ± 
ETF_POOL = {
    # å®½åŸºETF
    "510300": "æ²ªæ·±300ETF",
    "510500": "ä¸­è¯500ETF",
    "159915": "åˆ›ä¸šæ¿ETF",
    "510050": "ä¸Šè¯50ETF",
    "588000": "ç§‘åˆ›50ETF",
    "159901": "æ·±è¯100ETF",
    "512100": "ä¸­è¯1000ETF",
    # è¡Œä¸šETF
    "512480": "åŠå¯¼ä½“ETF",
    "515030": "æ–°èƒ½æºè½¦ETF",
    "512690": "ç™½é…’ETF",
    "512660": "å†›å·¥ETF",
    "512010": "åŒ»è¯ETF",
    "515790": "å…‰ä¼ETF",
    "512800": "é“¶è¡ŒETF",
    "512880": "è¯åˆ¸ETF",
    "515050": "5GETF",
    "512400": "æœ‰è‰²é‡‘å±ETF",
    "159825": "å†œä¸šETF",
    "512200": "æˆ¿åœ°äº§ETF",
    "516150": "ç¨€åœŸETF",
    "515220": "ç…¤ç‚­ETF",
    "159766": "æ—…æ¸¸ETF",
    "512580": "ç¯ä¿ETF",
    "512760": "èŠ¯ç‰‡ETF",
    "512720": "è®¡ç®—æœºETF",
    "159928": "æ¶ˆè´¹ETF",
    "512170": "åŒ»ç–—ETF",
    "159869": "æ¸¸æˆETF",
    "515000": "ç§‘æŠ€ETF",
    "512980": "ä¼ åª’ETF",
    "159740": "äººå·¥æ™ºèƒ½ETF",
    "512670": "å›½é˜²ETF",
    "516510": "äº‘è®¡ç®—ETF",
    "159607": "å‚¨èƒ½ETF",
    "159611": "ç”µåŠ›ETF",
}


class ETFHistoricalDataFetcher:
    """ETFå†å²æ•°æ®è·å–å™¨ - æ”¯æŒ3å¹´æ•°æ®"""
    
    def __init__(self):
        self.etf_pool = ETF_POOL
        self.retry_times = 3
        self.retry_delay = 2
    
    def _retry_fetch(self, func, *args, **kwargs):
        """å¸¦é‡è¯•çš„æ•°æ®è·å–"""
        for i in range(self.retry_times):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                logger.warning(f"æ•°æ®è·å–å¤±è´¥ (å°è¯• {i+1}/{self.retry_times}): {e}")
                if i < self.retry_times - 1:
                    time.sleep(self.retry_delay)
        return None
    
    def fetch_etf_history(self, etf_code: str, start_date: str = None, 
                          end_date: str = None) -> Optional[pd.DataFrame]:
        """
        è·å–å•åªETFå®Œæ•´å†å²æ•°æ®
        
        Args:
            etf_code: ETFä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        """
        try:
            df = self._retry_fetch(
                ak.fund_etf_hist_em,
                symbol=etf_code,
                period="daily",
                adjust="qfq"
            )
            
            if df is not None and not df.empty:
                df = df.rename(columns={
                    "æ—¥æœŸ": "date",
                    "å¼€ç›˜": "open",
                    "æ”¶ç›˜": "close",
                    "æœ€é«˜": "high",
                    "æœ€ä½": "low",
                    "æˆäº¤é‡": "volume",
                    "æˆäº¤é¢": "turnover",
                    "æŒ¯å¹…": "amplitude",
                    "æ¶¨è·Œå¹…": "change_pct",
                    "æ¶¨è·Œé¢": "change_amount",
                    "æ¢æ‰‹ç‡": "turnover_rate",
                })
                
                df["etf_code"] = etf_code
                df["etf_name"] = self.etf_pool.get(etf_code, etf_code)
                df["date"] = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d")
                
                # æ—¥æœŸè¿‡æ»¤
                if start_date:
                    df = df[df["date"] >= start_date]
                if end_date:
                    df = df[df["date"] <= end_date]
                
                return df
        except Exception as e:
            logger.warning(f"è·å–ETF {etf_code} å†å²æ•°æ®å¤±è´¥: {e}")
        
        return None
    
    def fetch_all_etf_history(self, years: int = 3) -> pd.DataFrame:
        """
        è·å–æ‰€æœ‰ETFæ± 3å¹´å†å²æ•°æ®
        
        Args:
            years: è·å–å¹´æ•°ï¼Œé»˜è®¤3å¹´
        """
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=years * 365)).strftime("%Y-%m-%d")
        
        logger.info(f"è·å–ETFå†å²æ•°æ®: {start_date} ~ {end_date} ({years}å¹´)")
        
        all_data = []
        total = len(self.etf_pool)
        
        for i, (etf_code, etf_name) in enumerate(self.etf_pool.items(), 1):
            logger.info(f"[{i}/{total}] è·å– {etf_name} ({etf_code})...")
            
            df = self.fetch_etf_history(etf_code, start_date, end_date)
            if df is not None and not df.empty:
                all_data.append(df)
                logger.info(f"   è·å– {len(df)} æ¡è®°å½•")
            
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        if all_data:
            result = pd.concat(all_data, ignore_index=True)
            logger.info(f"ETFå†å²æ•°æ®è·å–å®Œæˆ: {len(result)} æ¡, {len(self.etf_pool)} åªETF")
            return result
        
        return pd.DataFrame()
    
    def save_history_data(self, df: pd.DataFrame, filename: str = "etf_history_3y.csv"):
        """ä¿å­˜å†å²æ•°æ®"""
        save_path = PROJECT_ROOT / "data" / "historical" / filename
        save_path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(save_path, index=False, encoding="utf-8-sig")
        logger.info(f"ETFå†å²æ•°æ®å·²ä¿å­˜: {save_path}")
        return str(save_path)
    
    def load_history_data(self, filename: str = "etf_history_3y.csv") -> pd.DataFrame:
        """åŠ è½½å†å²æ•°æ®"""
        load_path = PROJECT_ROOT / "data" / "historical" / filename
        if load_path.exists():
            df = pd.read_csv(load_path)
            logger.info(f"ETFå†å²æ•°æ®å·²åŠ è½½: {len(df)} æ¡")
            return df
        return pd.DataFrame()


class ETFFeatureEngineer:
    """ETFç‰¹å¾å·¥ç¨‹ - å¢å¼ºç‰ˆ"""
    
    def __init__(self):
        self.feature_windows = [3, 5, 10, 20, 60]
        self.feature_columns = self.get_feature_columns()
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºETFç‰¹å¾æ•°æ®é›†
        """
        if df.empty or len(df) < 100:
            logger.warning("ETFæ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ›å»ºç‰¹å¾")
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼
        df = df.copy()
        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')
        df = df.sort_values(['etf_code', 'date']).reset_index(drop=True)
        
        all_features = []
        
        for etf_code in df['etf_code'].unique():
            etf_df = df[df['etf_code'] == etf_code].copy()
            
            if len(etf_df) < 60:  # éœ€è¦è¶³å¤Ÿå†å²æ•°æ®
                continue
            
            features = self._compute_single_etf_features(etf_df)
            all_features.append(features)
        
        if not all_features:
            return pd.DataFrame()
        
        result = pd.concat(all_features, ignore_index=True)
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆT+1æ—¥æ¶¨è·Œå¹…ï¼‰
        result = self._create_labels(result)
        
        # åˆ é™¤ç¼ºå¤±å€¼
        result = result.dropna(subset=['label_score'])
        
        logger.info(f"ETFç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(result)} æ¡æ•°æ®, {result['etf_code'].nunique()} åªETF")
        return result
    
    def _compute_single_etf_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å•åªETFçš„ç‰¹å¾"""
        result = df[['date', 'etf_code', 'etf_name', 'open', 'high', 'low', 
                     'close', 'volume', 'turnover', 'change_pct', 
                     'turnover_rate', 'amplitude']].copy()
        
        # 1. æ”¶ç›Šç‡ç‰¹å¾
        for window in self.feature_windows:
            result[f'return_{window}d'] = df['close'].pct_change(window) * 100
            result[f'volatility_{window}d'] = df['close'].pct_change().rolling(window).std() * 100
        
        # 2. å‡çº¿ç‰¹å¾
        for window in [5, 10, 20, 60]:
            result[f'ma{window}'] = df['close'].rolling(window).mean()
            result[f'ma{window}_bias'] = (df['close'] - result[f'ma{window}']) / result[f'ma{window}'] * 100
        
        # 3. å‡çº¿å¤šå¤´/ç©ºå¤´æ’åˆ—
        result['ma_bull'] = ((result['ma5'] > result['ma10']) & 
                            (result['ma10'] > result['ma20'])).astype(int)
        result['ma_bear'] = ((result['ma5'] < result['ma10']) & 
                            (result['ma10'] < result['ma20'])).astype(int)
        
        # 4. æˆäº¤é‡ç‰¹å¾
        for window in [5, 10, 20]:
            result[f'volume_ma{window}'] = df['volume'].rolling(window).mean()
        result['volume_ratio'] = df['volume'] / result['volume_ma5']
        result['volume_trend'] = (result['volume_ma5'] / result['volume_ma20']).fillna(1)
        
        # 5. ä»·æ ¼ä½ç½®ç‰¹å¾
        result['high_20d'] = df['high'].rolling(20).max()
        result['low_20d'] = df['low'].rolling(20).min()
        result['price_position_20d'] = (df['close'] - result['low_20d']) / (result['high_20d'] - result['low_20d'] + 1e-8)
        
        result['high_60d'] = df['high'].rolling(60).max()
        result['low_60d'] = df['low'].rolling(60).min()
        result['price_position_60d'] = (df['close'] - result['low_60d']) / (result['high_60d'] - result['low_60d'] + 1e-8)
        
        # 6. æŠ€æœ¯æŒ‡æ ‡
        result['rsi_6'] = self._compute_rsi(df['close'], 6)
        result['rsi_12'] = self._compute_rsi(df['close'], 12)
        result['rsi_24'] = self._compute_rsi(df['close'], 24)
        
        # 7. MACD
        ema12 = df['close'].ewm(span=12, adjust=False).mean()
        ema26 = df['close'].ewm(span=26, adjust=False).mean()
        result['macd'] = ema12 - ema26
        result['macd_signal'] = result['macd'].ewm(span=9, adjust=False).mean()
        result['macd_hist'] = result['macd'] - result['macd_signal']
        
        # 8. å¸ƒæ—å¸¦
        result['bb_mid'] = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        result['bb_upper'] = result['bb_mid'] + 2 * bb_std
        result['bb_lower'] = result['bb_mid'] - 2 * bb_std
        result['bb_width'] = (result['bb_upper'] - result['bb_lower']) / result['bb_mid']
        result['bb_position'] = (df['close'] - result['bb_lower']) / (result['bb_upper'] - result['bb_lower'] + 1e-8)
        
        # 9. ATR (å¹³å‡çœŸå®æ³¢å¹…)
        result['atr_14'] = self._compute_atr(df, 14)
        result['atr_ratio'] = result['atr_14'] / df['close'] * 100
        
        # 10. åŠ¨é‡æŒ‡æ ‡
        result['momentum_10'] = df['close'] - df['close'].shift(10)
        result['momentum_20'] = df['close'] - df['close'].shift(20)
        result['roc_10'] = df['close'].pct_change(10) * 100
        result['roc_20'] = df['close'].pct_change(20) * 100
        
        # 11. é‡ä»·èƒŒç¦»ç‰¹å¾
        result['volume_rank'] = df['volume'].rolling(20).apply(lambda x: pd.Series(x).rank(pct=True).iloc[-1])
        result['price_rank'] = df['close'].pct_change().rolling(20).apply(lambda x: pd.Series(x).rank(pct=True).iloc[-1])
        result['divergence_score'] = result['volume_rank'] - result['price_rank']
        
        # 12. è¶‹åŠ¿å¼ºåº¦
        result['adx'] = self._compute_adx(df, 14)
        
        # 13. æ—¥å†…ç‰¹å¾
        result['upper_shadow'] = (df['high'] - df[['open', 'close']].max(axis=1)) / df['close'] * 100
        result['lower_shadow'] = (df[['open', 'close']].min(axis=1) - df['low']) / df['close'] * 100
        result['body_size'] = abs(df['close'] - df['open']) / df['close'] * 100
        
        # 14. è¿ç»­æ¶¨è·Œç‰¹å¾
        result['up_streak'] = self._compute_streak(df['change_pct'], True)
        result['down_streak'] = self._compute_streak(df['change_pct'], False)
        
        # 15. åè½¬ç‰¹å¾ï¼ˆè¶…è·Œåå¼¹ï¼‰
        result['oversold_signal'] = ((result['rsi_6'] < 30) & (result['price_position_20d'] < 0.2)).astype(int)
        result['overbought_signal'] = ((result['rsi_6'] > 70) & (result['price_position_20d'] > 0.8)).astype(int)
        
        # 16. åŠ¨é‡åè½¬
        result['momentum_reversal_5d'] = np.where(
            (result['return_5d'] < -3) & (df['change_pct'] > 0), 1,
            np.where((result['return_5d'] > 3) & (df['change_pct'] < 0), -1, 0)
        )
        
        # 17. æˆäº¤é‡å¼‚å¸¸
        result['volume_spike'] = (result['volume_ratio'] > 2).astype(int)
        result['volume_dry'] = (result['volume_ratio'] < 0.5).astype(int)
        
        # 18. æ³¢åŠ¨ç‡å˜åŒ–
        result['volatility_change'] = result['volatility_5d'] / (result['volatility_20d'] + 1e-8)
        
        # 19. è¶‹åŠ¿ä¸åŠ¨é‡ç»„åˆ
        result['trend_momentum'] = result['ma5_bias'] * result['momentum_10']
        
        # 20. ä»·æ ¼æ•ˆç‡
        result['price_efficiency'] = abs(df['close'] - df['close'].shift(10)) / (
            (df['high'].rolling(10).max() - df['low'].rolling(10).min()) + 1e-8)
        
        return result
    
    def _compute_streak(self, change_pct: pd.Series, is_up: bool) -> pd.Series:
        """è®¡ç®—è¿ç»­æ¶¨è·Œå¤©æ•°"""
        if is_up:
            condition = change_pct > 0
        else:
            condition = change_pct < 0
        
        streak = pd.Series(0, index=change_pct.index)
        current_streak = 0
        
        for i in range(len(change_pct)):
            if condition.iloc[i]:
                current_streak += 1
            else:
                current_streak = 0
            streak.iloc[i] = current_streak
        
        return streak
    
    def _compute_rsi(self, prices: pd.Series, window: int) -> pd.Series:
        """è®¡ç®—RSI"""
        delta = prices.diff()
        gain = delta.where(delta > 0, 0).rolling(window).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window).mean()
        rs = gain / (loss + 1e-8)
        return 100 - (100 / (1 + rs))
    
    def _compute_atr(self, df: pd.DataFrame, window: int) -> pd.Series:
        """è®¡ç®—ATR"""
        high = df['high']
        low = df['low']
        close = df['close']
        
        tr1 = high - low
        tr2 = abs(high - close.shift(1))
        tr3 = abs(low - close.shift(1))
        tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)
        
        return tr.rolling(window).mean()
    
    def _compute_adx(self, df: pd.DataFrame, window: int = 14) -> pd.Series:
        """è®¡ç®—ADXè¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡"""
        high = df['high']
        low = df['low']
        close = df['close']
        
        plus_dm = high.diff()
        minus_dm = -low.diff()
        
        plus_dm[plus_dm < 0] = 0
        minus_dm[minus_dm < 0] = 0
        
        tr = self._compute_atr(df, 1) * window
        
        plus_di = 100 * (plus_dm.rolling(window).mean() / (tr + 1e-8))
        minus_di = 100 * (minus_dm.rolling(window).mean() / (tr + 1e-8))
        
        dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di + 1e-8)
        adx = dx.rolling(window).mean()
        
        return adx
    
    def _create_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ›å»ºæ ‡ç­¾"""
        df = df.sort_values(['etf_code', 'date']).reset_index(drop=True)
        
        # T+1æ—¥æ¶¨è·Œå¹…ä½œä¸ºæ ‡ç­¾
        df['label_next_return'] = df.groupby('etf_code')['change_pct'].shift(-1)
        
        # å°†æ¶¨è·Œå¹…è½¬æ¢ä¸ºæ’ååˆ†æ•°ï¼ˆ0-1ï¼‰
        df['label_score'] = df.groupby('date')['label_next_return'].transform(
            lambda x: (x.rank() - 1) / (len(x) - 1) if len(x) > 1 else 0.5
        )
        
        return df
    
    def get_feature_columns(self) -> List[str]:
        """è·å–æ‰€æœ‰ç‰¹å¾åˆ—å"""
        features = []
        
        # æ”¶ç›Šç‡å’Œæ³¢åŠ¨ç‡ç‰¹å¾
        for window in [3, 5, 10, 20, 60]:
            features.extend([f'return_{window}d', f'volatility_{window}d'])
        
        # å‡çº¿åç¦»
        for window in [5, 10, 20, 60]:
            features.append(f'ma{window}_bias')
        
        features.extend([
            'ma_bull', 'ma_bear',
            'volume_ratio', 'volume_trend',
            'price_position_20d', 'price_position_60d',
            'rsi_6', 'rsi_12', 'rsi_24',
            'macd', 'macd_signal', 'macd_hist',
            'bb_width', 'bb_position',
            'atr_ratio',
            'momentum_10', 'momentum_20', 'roc_10', 'roc_20',
            'divergence_score',
            'adx',
            'upper_shadow', 'lower_shadow', 'body_size',
            'change_pct', 'turnover_rate', 'amplitude',
            # æ–°å¢ç‰¹å¾
            'up_streak', 'down_streak',
            'oversold_signal', 'overbought_signal',
            'momentum_reversal_5d',
            'volume_spike', 'volume_dry',
            'volatility_change',
            'trend_momentum',
            'price_efficiency',
        ])
        
        return features


class ETFRollingBacktestEngine:
    """ETFæ»šåŠ¨å›æµ‹å¼•æ“ - 3å¹´è®­ç»ƒ"""
    
    def __init__(self, 
                 train_window_months: int = 24,  # 2å¹´è®­ç»ƒ
                 valid_window_months: int = 6,   # 6ä¸ªæœˆéªŒè¯
                 step_months: int = 1):          # æ¯æœˆæ»šåŠ¨
        """
        Args:
            train_window_months: è®­ç»ƒçª—å£ï¼ˆæœˆï¼‰
            valid_window_months: éªŒè¯çª—å£ï¼ˆæœˆï¼‰
            step_months: æ»šåŠ¨æ­¥é•¿ï¼ˆæœˆï¼‰
        """
        self.train_window_months = train_window_months
        self.valid_window_months = valid_window_months
        self.step_months = step_months
        
        self.feature_engineer = ETFFeatureEngineer()
        self.results = []
        
        # ä¼˜åŒ–çš„LightGBMå‚æ•°
        self.lgbm_params = {
            "objective": "lambdarank",  # ä½¿ç”¨æ’åºç›®æ ‡
            "metric": "ndcg",
            "ndcg_eval_at": [5],
            "boosting_type": "gbdt",
            "num_leaves": 63,
            "learning_rate": 0.03,
            "feature_fraction": 0.7,
            "bagging_fraction": 0.7,
            "bagging_freq": 5,
            "lambda_l1": 0.1,
            "lambda_l2": 0.5,
            "min_data_in_leaf": 30,
            "max_depth": 8,
            "verbose": -1,
            "seed": 42,
            "n_estimators": 500,
            "early_stopping_rounds": 50,
        }
    
    def _get_feature_columns(self, df: pd.DataFrame) -> List[str]:
        """è·å–å¯ç”¨çš„ç‰¹å¾åˆ—"""
        exclude_cols = ['date', 'etf_code', 'etf_name', 'open', 'close', 
                        'high', 'low', 'volume', 'turnover',
                        'ma5', 'ma10', 'ma20', 'ma60',
                        'volume_ma5', 'volume_ma10', 'volume_ma20',
                        'high_20d', 'low_20d', 'high_60d', 'low_60d',
                        'bb_mid', 'bb_upper', 'bb_lower',
                        'volume_rank', 'price_rank',
                        'label_next_return', 'label_score']
        
        return [c for c in df.columns 
                if c not in exclude_cols 
                and df[c].dtype in ['float64', 'int64', 'float32', 'int32']]
    
    def run_backtest(self, df_history: pd.DataFrame,
                     test_start_date: str,
                     test_end_date: str) -> pd.DataFrame:
        """
        è¿è¡ŒETFæ»šåŠ¨å›æµ‹
        
        Args:
            df_history: ETFå†å²æ•°æ®
            test_start_date: æµ‹è¯•å¼€å§‹æ—¥æœŸ
            test_end_date: æµ‹è¯•ç»“æŸæ—¥æœŸ
        """
        logger.info(f"å¼€å§‹ETFæ»šåŠ¨å›æµ‹: {test_start_date} -> {test_end_date}")
        logger.info(f"è®­ç»ƒçª—å£: {self.train_window_months}ä¸ªæœˆ")
        
        # åˆ›å»ºç‰¹å¾
        logger.info("åˆ›å»ºETFç‰¹å¾æ•°æ®é›†...")
        df = self.feature_engineer.create_features(df_history)
        
        if df.empty:
            logger.error("ETFç‰¹å¾æ•°æ®ä¸ºç©º!")
            return pd.DataFrame()
        
        # è·å–ç‰¹å¾åˆ—
        feature_cols = self._get_feature_columns(df)
        logger.info(f"ä½¿ç”¨ç‰¹å¾: {len(feature_cols)} ä¸ª")
        
        # è·å–æµ‹è¯•æ—¥æœŸ
        test_dates = df[(df['date'] >= test_start_date) & 
                        (df['date'] <= test_end_date)]['date'].unique()
        test_dates = sorted(test_dates)
        
        logger.info(f"æµ‹è¯•æ—¥æœŸ: {len(test_dates)} å¤©")
        
        all_predictions = []
        current_month = None
        model = None
        
        for test_date in test_dates:
            test_month = test_date[:7]
            
            # æ¯æœˆé‡æ–°è®­ç»ƒ
            if test_month != current_month:
                current_month = test_month
                
                # è®¡ç®—è®­ç»ƒçª—å£
                train_end = datetime.strptime(test_date, '%Y-%m-%d') - timedelta(days=1)
                train_start = train_end - timedelta(days=self.train_window_months * 30)
                
                train_end_str = train_end.strftime('%Y-%m-%d')
                train_start_str = train_start.strftime('%Y-%m-%d')
                
                # è·å–è®­ç»ƒæ•°æ®
                train_df = df[(df['date'] >= train_start_str) & 
                              (df['date'] <= train_end_str)]
                
                if len(train_df) < 500:
                    logger.warning(f"è®­ç»ƒæ•°æ®ä¸è¶³: {len(train_df)}, è·³è¿‡ {test_month}")
                    continue
                
                # è®­ç»ƒæ¨¡å‹
                model = self._train_model(train_df, feature_cols)
                logger.info(f"[{test_month}] è®­ç»ƒå®Œæˆ, æ ·æœ¬: {len(train_df)}")
            
            if model is None:
                continue
            
            # é¢„æµ‹å½“å¤©
            test_df = df[df['date'] == test_date]
            if test_df.empty:
                continue
            
            predictions = self._predict(model, test_df, feature_cols)
            all_predictions.append(predictions)
        
        if not all_predictions:
            return pd.DataFrame()
        
        results = pd.concat(all_predictions, ignore_index=True)
        
        # è¯„ä¼°ç»“æœ
        self._evaluate_results(results)
        
        # ä¿å­˜ç»“æœ
        self._save_results(results)
        
        return results
    
    def _train_model(self, train_df: pd.DataFrame, 
                     feature_cols: List[str]) -> lgb.Booster:
        """è®­ç»ƒLightGBMæ¨¡å‹ - ä¼˜åŒ–ç‰ˆV2
        
        æ ¸å¿ƒæ”¹è¿›ï¼š
        1. ä½¿ç”¨æ›´å¤šæ•°æ®è®­ç»ƒï¼ˆå–æ¶ˆè¿‡æ—©early stoppingï¼‰
        2. æ·»åŠ åˆ†ç±»æ¦‚å¿µï¼ˆæ¶¨/è·Œï¼‰
        3. æ›´åˆç†çš„éªŒè¯é›†æ¯”ä¾‹
        """
        # ä½¿ç”¨åŸå§‹æ”¶ç›Šç‡ä½œä¸ºæ ‡ç­¾
        X = train_df[feature_cols].fillna(0)
        y = train_df['label_next_return'].fillna(0)
        
        # æŒ‰æ—¶é—´åˆ†å‰²ï¼ˆ90%è®­ç»ƒï¼Œ10%éªŒè¯ï¼‰
        dates = sorted(train_df['date'].unique())
        split_date = dates[int(len(dates) * 0.9)]
        
        train_mask = train_df['date'] < split_date
        valid_mask = train_df['date'] >= split_date
        
        X_train = X[train_mask]
        y_train = y[train_mask]
        X_valid = X[valid_mask]
        y_valid = y[valid_mask]
        
        # åˆ›å»ºæ•°æ®é›†
        train_data = lgb.Dataset(X_train, label=y_train)
        valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)
        
        # æ›´å¼ºçš„è®­ç»ƒå‚æ•°
        params = {
            "objective": "regression",
            "metric": "mse",
            "boosting_type": "gbdt",
            "num_leaves": 127,           # å¢åŠ å¤æ‚åº¦
            "learning_rate": 0.05,       # é€‚ä¸­å­¦ä¹ ç‡
            "feature_fraction": 0.8,     # ç‰¹å¾é‡‡æ ·
            "bagging_fraction": 0.8,     # æ•°æ®é‡‡æ ·
            "bagging_freq": 3,
            "lambda_l1": 0.05,           # å‡å°æ­£åˆ™åŒ–
            "lambda_l2": 0.2,            # å‡å°æ­£åˆ™åŒ–
            "min_data_in_leaf": 20,      # å‡å°å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬
            "max_depth": 10,             # å¢åŠ æ·±åº¦
            "min_gain_to_split": 0.001,
            "verbose": -1,
            "seed": 42,
            "num_threads": 4,
            "force_col_wise": True,
        }
        
        model = lgb.train(
            params,
            train_data,
            valid_sets=[train_data, valid_data],
            valid_names=['train', 'valid'],
            num_boost_round=300,         # å›ºå®šè½®æ•°
            callbacks=[
                lgb.early_stopping(200),  # æ”¾å®½early stopping
                lgb.log_evaluation(period=0)
            ]
        )
        
        return model
    
    def _predict(self, model: lgb.Booster, test_df: pd.DataFrame,
                 feature_cols: List[str]) -> pd.DataFrame:
        """é¢„æµ‹ - çº¯åŠ¨é‡ç­–ç•¥ + æ¨¡å‹è¾…åŠ©
        
        æ ¸å¿ƒæ€è·¯ï¼ˆåŸºäºåŠ¨é‡æ•ˆåº”ï¼‰ï¼š
        1. ä¸­æœŸåŠ¨é‡ï¼ˆ10-20æ—¥ï¼‰æ˜¯æ ¸å¿ƒ
        2. çŸ­æœŸåŠ¨é‡ï¼ˆ3-5æ—¥ï¼‰ç¡®è®¤
        3. æ¨¡å‹æ’é™¤æç«¯é£é™©
        4. æˆäº¤é‡ç¡®è®¤è¶‹åŠ¿æœ‰æ•ˆæ€§
        """
        X = test_df[feature_cols].fillna(0)
        model_scores = model.predict(X)
        
        result = test_df[['date', 'etf_code', 'etf_name', 'label_next_return']].copy()
        result['model_score'] = model_scores
        
        # 1. ä¸­æœŸåŠ¨é‡ (10æ—¥æ¶¨å¹…) - æ ¸å¿ƒå› å­
        if 'return_10d' in test_df.columns:
            return_10d = test_df['return_10d'].values
            result['momentum_10d'] = pd.Series(return_10d).rank(pct=True).values
        else:
            result['momentum_10d'] = 0.5
        
        # 2. çŸ­æœŸåŠ¨é‡ (5æ—¥æ¶¨å¹…) - ç¡®è®¤å› å­
        if 'return_5d' in test_df.columns:
            return_5d = test_df['return_5d'].values
            result['momentum_5d'] = pd.Series(return_5d).rank(pct=True).values
        else:
            result['momentum_5d'] = 0.5
        
        # 3. æˆäº¤é‡è¶‹åŠ¿ - é‡ä»·é…åˆ
        if 'volume_trend' in test_df.columns:
            vol_trend = test_df['volume_trend'].values
            result['vol_score'] = pd.Series(vol_trend).rank(pct=True).values
        else:
            result['vol_score'] = 0.5
        
        # 4. æ¨¡å‹åˆ†æ•°å½’ä¸€åŒ– - è¿‡æ»¤ç”¨
        min_score = result['model_score'].min()
        max_score = result['model_score'].max()
        if max_score > min_score:
            result['model_norm'] = (result['model_score'] - min_score) / (max_score - min_score)
        else:
            result['model_norm'] = 0.5
        
        # ç»¼åˆåˆ†æ•°ï¼šåŠ¨é‡ä¸»å¯¼
        # 10æ—¥åŠ¨é‡ 45% + 5æ—¥åŠ¨é‡ 30% + æ¨¡å‹ 15% + æˆäº¤é‡ 10%
        result['pred_score'] = (
            0.45 * result['momentum_10d'] +
            0.30 * result['momentum_5d'] +
            0.15 * result['model_norm'] +
            0.10 * result['vol_score']
        )
        
        result['pred_rank'] = result['pred_score'].rank(ascending=False)
        result['actual_rank'] = result['label_next_return'].rank(ascending=False)
        
        return result
    
    def _evaluate_results(self, results: pd.DataFrame):
        """è¯„ä¼°å›æµ‹ç»“æœ"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ETFå›æµ‹ç»“æœè¯„ä¼°")
        logger.info("=" * 60)
        
        # æŒ‰æ—¥æœŸåˆ†ç»„è¯„ä¼°
        daily_stats = []
        
        for date in results['date'].unique():
            day_df = results[results['date'] == date]
            
            # Top5é¢„æµ‹
            top5_pred = day_df.nsmallest(5, 'pred_rank')
            top5_actual_return = top5_pred['label_next_return'].mean()
            
            # åŸºå‡†æ”¶ç›Š
            benchmark_return = day_df['label_next_return'].mean()
            
            # å‘½ä¸­ç‡ï¼ˆé¢„æµ‹Top5æ˜¯å¦åœ¨å®é™…Top10ï¼‰
            actual_top10 = day_df.nsmallest(10, 'actual_rank')['etf_code'].tolist()
            hit_count = sum(1 for etf in top5_pred['etf_code'] if etf in actual_top10)
            
            daily_stats.append({
                'date': date,
                'top5_return': top5_actual_return,
                'benchmark_return': benchmark_return,
                'excess_return': top5_actual_return - benchmark_return,
                'hit_count': hit_count,
                'hit_rate': hit_count / 5,
            })
        
        stats_df = pd.DataFrame(daily_stats)
        
        # æ±‡æ€»ç»Ÿè®¡
        logger.info(f"\nğŸ“… ç»Ÿè®¡å‘¨æœŸ: {stats_df['date'].min()} ~ {stats_df['date'].max()}")
        logger.info(f"ğŸ“ˆ æ€»äº¤æ˜“å¤©æ•°: {len(stats_df)}")
        logger.info(f"ğŸ¯ å¹³å‡å‘½ä¸­ç‡: {stats_df['hit_rate'].mean():.2%}")
        logger.info(f"ğŸ’° Top5å¹³å‡æ—¥æ”¶ç›Š: {stats_df['top5_return'].mean():.3f}%")
        logger.info(f"ğŸ“Š åŸºå‡†å¹³å‡æ—¥æ”¶ç›Š: {stats_df['benchmark_return'].mean():.3f}%")
        logger.info(f"ğŸ’ å¹³å‡è¶…é¢æ”¶ç›Š: {stats_df['excess_return'].mean():.3f}%")
        logger.info(f"ğŸ“ˆ ç´¯è®¡è¶…é¢æ”¶ç›Š: {stats_df['excess_return'].sum():.2f}%")
        
        # èƒœç‡
        win_days = (stats_df['excess_return'] > 0).sum()
        logger.info(f"ğŸ† è¶…é¢æ”¶ç›Šèƒœç‡: {win_days}/{len(stats_df)} ({win_days/len(stats_df):.2%})")
        
        # æœ€å¤§å›æ’¤
        cumulative = stats_df['excess_return'].cumsum()
        max_drawdown = (cumulative.cummax() - cumulative).max()
        logger.info(f"ğŸ“‰ æœ€å¤§å›æ’¤: {max_drawdown:.2f}%")
        
        # å¤æ™®æ¯”ç‡
        daily_std = stats_df['excess_return'].std()
        if daily_std > 0:
            sharpe = stats_df['excess_return'].mean() / daily_std * np.sqrt(252)
            logger.info(f"ğŸ“Š å¤æ™®æ¯”ç‡: {sharpe:.2f}")
        
        logger.info("=" * 60)
        
        self.backtest_stats = stats_df
    
    def _save_results(self, results: pd.DataFrame):
        """ä¿å­˜å›æµ‹ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        result_path = BACKTEST_RESULTS_DIR / f"etf_backtest_detail_{timestamp}.csv"
        results.to_csv(result_path, index=False, encoding='utf-8-sig')
        logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {result_path}")
        
        # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
        if hasattr(self, 'backtest_stats'):
            stats_path = BACKTEST_RESULTS_DIR / f"etf_backtest_stats_{timestamp}.csv"
            self.backtest_stats.to_csv(stats_path, index=False, encoding='utf-8-sig')
            logger.info(f"ç»Ÿè®¡ç»“æœå·²ä¿å­˜: {stats_path}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_report(results, timestamp)
    
    def _generate_report(self, results: pd.DataFrame, timestamp: str):
        """ç”Ÿæˆå›æµ‹æŠ¥å‘Š"""
        stats = self.backtest_stats if hasattr(self, 'backtest_stats') else None
        
        lines = [
            "# ğŸ“Š ETFé¢„æµ‹ç³»ç»Ÿå›æµ‹æŠ¥å‘Š",
            "",
            f"**å›æµ‹æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**è®­ç»ƒçª—å£**: {self.train_window_months} ä¸ªæœˆ",
            "",
            "---",
            "",
            "## ğŸ“ˆ ç»¼åˆç»©æ•ˆ",
            "",
        ]
        
        if stats is not None:
            lines.extend([
                f"| æŒ‡æ ‡ | æ•°å€¼ |",
                f"|------|------|",
                f"| ç»Ÿè®¡å‘¨æœŸ | {stats['date'].min()} ~ {stats['date'].max()} |",
                f"| äº¤æ˜“å¤©æ•° | {len(stats)} |",
                f"| å¹³å‡å‘½ä¸­ç‡ | {stats['hit_rate'].mean():.2%} |",
                f"| Top5å¹³å‡æ—¥æ”¶ç›Š | {stats['top5_return'].mean():.3f}% |",
                f"| åŸºå‡†å¹³å‡æ—¥æ”¶ç›Š | {stats['benchmark_return'].mean():.3f}% |",
                f"| å¹³å‡è¶…é¢æ”¶ç›Š | {stats['excess_return'].mean():.3f}% |",
                f"| ç´¯è®¡è¶…é¢æ”¶ç›Š | {stats['excess_return'].sum():.2f}% |",
                f"| è¶…é¢æ”¶ç›Šèƒœç‡ | {(stats['excess_return'] > 0).mean():.2%} |",
                "",
            ])
        
        lines.extend([
            "---",
            "",
            "## ğŸ”§ æ¨¡å‹å‚æ•°",
            "",
            "```",
            f"è®­ç»ƒçª—å£: {self.train_window_months} ä¸ªæœˆ",
            f"æ»šåŠ¨æ­¥é•¿: {self.step_months} ä¸ªæœˆ",
            f"æ¨¡å‹: LightGBM",
            f"ç‰¹å¾æ•°: {len(self.feature_engineer.get_feature_columns())}",
            "```",
            "",
            "---",
            "",
            "*æŠ¥å‘Šç”± ETFé¢„æµ‹ç³»ç»Ÿ è‡ªåŠ¨ç”Ÿæˆ*",
        ])
        
        report_content = "\n".join(lines)
        report_path = BACKTEST_RESULTS_DIR / f"etf_backtest_report_{timestamp}.md"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        
        logger.info(f"å›æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
