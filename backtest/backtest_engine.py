"""
Aè‚¡æ¿å—æ¶¨åœé¢„æµ‹ç³»ç»Ÿ - æ»šåŠ¨å›æµ‹å¼•æ“
çœŸæ­£çš„å†å²å›æµ‹éªŒè¯ï¼šç”¨è¿‡å»æ•°æ®è®­ç»ƒï¼Œé¢„æµ‹æœªæ¥æ•°æ®
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Tuple
import logging
from pathlib import Path
import pickle
import json
import warnings

import lightgbm as lgb
from sklearn.metrics import ndcg_score, mean_squared_error

warnings.filterwarnings('ignore')

PROJECT_ROOT = Path(__file__).parent.parent
HISTORICAL_DATA_DIR = PROJECT_ROOT / "data" / "historical"
BACKTEST_RESULTS_DIR = PROJECT_ROOT / "data" / "backtest_results"
BACKTEST_RESULTS_DIR.mkdir(parents=True, exist_ok=True)

logger = logging.getLogger(__name__)


class HistoricalFeatureEngineer:
    """å†å²æ•°æ®ç‰¹å¾å·¥ç¨‹"""
    
    def __init__(self):
        self.feature_columns = [
            # åŸºç¡€è¡Œæƒ…ç‰¹å¾
            "change_pct",           # å½“æ—¥æ¶¨è·Œå¹…
            "turnover_rate",        # æ¢æ‰‹ç‡
            "amplitude",            # æŒ¯å¹…
            
            # æ—¶é—´åºåˆ—ç‰¹å¾
            "change_pct_ma3",       # 3æ—¥æ¶¨å¹…å‡çº¿
            "change_pct_ma5",       # 5æ—¥æ¶¨å¹…å‡çº¿
            "change_pct_ma10",      # 10æ—¥æ¶¨å¹…å‡çº¿
            "change_pct_std5",      # 5æ—¥æ¶¨å¹…æ³¢åŠ¨ç‡
            "change_pct_std10",     # 10æ—¥æ¶¨å¹…æ³¢åŠ¨ç‡
            
            # åŠ¨é‡ç‰¹å¾
            "momentum_3d",          # 3æ—¥åŠ¨é‡
            "momentum_5d",          # 5æ—¥åŠ¨é‡
            "momentum_10d",         # 10æ—¥åŠ¨é‡
            "momentum_20d",         # 20æ—¥åŠ¨é‡
            
            # æ¶¨åœç›¸å…³ç‰¹å¾
            "limit_up_count",       # å½“æ—¥æ¶¨åœå®¶æ•°
            "limit_up_count_ma3",   # 3æ—¥æ¶¨åœå‡çº¿
            "limit_up_count_ma5",   # 5æ—¥æ¶¨åœå‡çº¿
            "limit_up_rank",        # æ¶¨åœå®¶æ•°æ’å
            
            # æˆäº¤é¢ç‰¹å¾
            "turnover_ma5",         # 5æ—¥æˆäº¤é¢å‡çº¿
            "turnover_ma10",        # 10æ—¥æˆäº¤é¢å‡çº¿
            "volume_ratio",         # é‡æ¯”
            
            # è¶‹åŠ¿ç‰¹å¾
            "above_ma5",            # ä»·æ ¼åœ¨5æ—¥å‡çº¿ä¸Šæ–¹
            "above_ma10",           # ä»·æ ¼åœ¨10æ—¥å‡çº¿ä¸Šæ–¹
            "above_ma20",           # ä»·æ ¼åœ¨20æ—¥å‡çº¿ä¸Šæ–¹
            
            # é‡ä»·èƒŒç¦»ç‰¹å¾ï¼ˆæ ¸å¿ƒï¼‰
            "divergence_score",     # é‡ä»·èƒŒç¦»è¯„åˆ†
        ]
    
    def create_features(self, sector_history: pd.DataFrame, 
                        limit_up_agg: pd.DataFrame) -> pd.DataFrame:
        """
        åˆ›å»ºç‰¹å¾æ•°æ®é›†
        
        Args:
            sector_history: æ¿å—å†å²è¡Œæƒ…æ•°æ®
            limit_up_agg: æŒ‰æ¿å—èšåˆçš„æ¶¨åœæ•°æ®
            
        Returns:
            åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„DataFrame
        """
        if sector_history.empty:
            return pd.DataFrame()
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´
        sector_history = sector_history.copy()
        sector_history['date'] = pd.to_datetime(sector_history['date']).dt.strftime('%Y-%m-%d')
        
        # æŒ‰æ¿å—åˆ†ç»„å¤„ç†
        all_features = []
        
        for sector_name in sector_history['sector_name'].unique():
            sector_df = sector_history[sector_history['sector_name'] == sector_name].copy()
            sector_df = sector_df.sort_values('date').reset_index(drop=True)
            
            if len(sector_df) < 30:  # éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®
                continue
            
            # åŸºç¡€ç‰¹å¾
            features = sector_df[['date', 'sector_name', 'sector_type', 
                                   'open', 'close', 'high', 'low', 
                                   'volume', 'turnover', 'change_pct', 
                                   'turnover_rate', 'amplitude']].copy()
            
            # æ—¶é—´åºåˆ—ç‰¹å¾
            features['change_pct_ma3'] = sector_df['change_pct'].rolling(3).mean()
            features['change_pct_ma5'] = sector_df['change_pct'].rolling(5).mean()
            features['change_pct_ma10'] = sector_df['change_pct'].rolling(10).mean()
            features['change_pct_std5'] = sector_df['change_pct'].rolling(5).std()
            features['change_pct_std10'] = sector_df['change_pct'].rolling(10).std()
            
            # åŠ¨é‡ç‰¹å¾
            features['momentum_3d'] = sector_df['close'].pct_change(3) * 100
            features['momentum_5d'] = sector_df['close'].pct_change(5) * 100
            features['momentum_10d'] = sector_df['close'].pct_change(10) * 100
            features['momentum_20d'] = sector_df['close'].pct_change(20) * 100
            
            # æˆäº¤é¢ç‰¹å¾
            features['turnover_ma5'] = sector_df['turnover'].rolling(5).mean()
            features['turnover_ma10'] = sector_df['turnover'].rolling(10).mean()
            features['volume_ratio'] = sector_df['turnover'] / features['turnover_ma5']
            
            # å‡çº¿ä½ç½®ç‰¹å¾
            features['ma5'] = sector_df['close'].rolling(5).mean()
            features['ma10'] = sector_df['close'].rolling(10).mean()
            features['ma20'] = sector_df['close'].rolling(20).mean()
            features['above_ma5'] = (sector_df['close'] > features['ma5']).astype(int)
            features['above_ma10'] = (sector_df['close'] > features['ma10']).astype(int)
            features['above_ma20'] = (sector_df['close'] > features['ma20']).astype(int)
            
            # é‡ä»·èƒŒç¦»è¯„åˆ†ï¼ˆæ ¸å¿ƒç‰¹å¾ï¼‰
            # èµ„é‡‘æµå…¥æ’åé«˜ä½†æ¶¨å¹…æ’åä½ = å¸ç­¹ä¿¡å·
            features['turnover_rank'] = features['turnover'].rank(pct=True)
            features['change_rank'] = features['change_pct'].rank(pct=True)
            features['divergence_score'] = features['turnover_rank'] - features['change_rank']
            
            all_features.append(features)
        
        if not all_features:
            return pd.DataFrame()
        
        result = pd.concat(all_features, ignore_index=True)
        
        # åˆå¹¶æ¶¨åœæ•°æ®
        if not limit_up_agg.empty:
            limit_up_agg = limit_up_agg.copy()
            limit_up_agg['date'] = pd.to_datetime(limit_up_agg['date'].astype(str)).dt.strftime('%Y-%m-%d')
            result = result.merge(
                limit_up_agg[['date', 'sector_name', 'limit_up_count', 
                              'total_seal_amount', 'max_continuous_limit_up']],
                on=['date', 'sector_name'],
                how='left'
            )
            result['limit_up_count'] = result['limit_up_count'].fillna(0)
        else:
            result['limit_up_count'] = 0
            result['total_seal_amount'] = 0
            result['max_continuous_limit_up'] = 0
        
        # æ¶¨åœå®¶æ•°æ—¶åºç‰¹å¾
        for sector_name in result['sector_name'].unique():
            mask = result['sector_name'] == sector_name
            result.loc[mask, 'limit_up_count_ma3'] = result.loc[mask, 'limit_up_count'].rolling(3).mean()
            result.loc[mask, 'limit_up_count_ma5'] = result.loc[mask, 'limit_up_count'].rolling(5).mean()
        
        # æ¯æ—¥æ¶¨åœæ’å
        for date in result['date'].unique():
            mask = result['date'] == date
            result.loc[mask, 'limit_up_rank'] = result.loc[mask, 'limit_up_count'].rank(ascending=False)
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆT+1æ—¥æ¶¨å¹…å’Œæ¶¨åœæ•°ï¼‰
        result = result.sort_values(['sector_name', 'date'])
        result['label_change_pct'] = result.groupby('sector_name')['change_pct'].shift(-1)
        result['label_limit_up_count'] = result.groupby('sector_name')['limit_up_count'].shift(-1)
        
        # ç»¼åˆæ ‡ç­¾ï¼šæ¶¨å¹… + æ¶¨åœå¥–åŠ±
        result['label_score'] = result['label_change_pct'] + result['label_limit_up_count'] * 2
        
        # åˆ é™¤ç¼ºå¤±å€¼
        result = result.dropna(subset=['label_score'])
        
        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(result)} æ¡æ•°æ®, {result['sector_name'].nunique()} ä¸ªæ¿å—")
        return result


class RollingBacktestEngine:
    """æ»šåŠ¨å›æµ‹å¼•æ“"""
    
    def __init__(self, train_window_months: int = 24, 
                 valid_window_months: int = 3,
                 step_months: int = 1):
        """
        Args:
            train_window_months: è®­ç»ƒçª—å£ï¼ˆæœˆï¼‰
            valid_window_months: éªŒè¯çª—å£ï¼ˆæœˆï¼‰
            step_months: æ»šåŠ¨æ­¥é•¿ï¼ˆæœˆï¼‰
        """
        self.train_window_months = train_window_months
        self.valid_window_months = valid_window_months
        self.step_months = step_months
        
        self.feature_engineer = HistoricalFeatureEngineer()
        self.results = []
        
        # LightGBMå‚æ•°
        self.lgbm_params = {
            "objective": "regression",
            "metric": "mse",
            "boosting_type": "gbdt",
            "num_leaves": 31,
            "learning_rate": 0.05,
            "feature_fraction": 0.8,
            "bagging_fraction": 0.8,
            "bagging_freq": 5,
            "lambda_l1": 0.1,
            "lambda_l2": 0.1,
            "min_data_in_leaf": 20,
            "verbose": -1,
            "seed": 42,
        }
    
    def _get_feature_columns(self, df: pd.DataFrame) -> List[str]:
        """è·å–å¯ç”¨çš„ç‰¹å¾åˆ—"""
        exclude_cols = ['date', 'sector_name', 'sector_type', 'open', 'close', 
                        'high', 'low', 'volume', 'turnover', 'ma5', 'ma10', 'ma20',
                        'turnover_rank', 'change_rank',
                        'label_change_pct', 'label_limit_up_count', 'label_score']
        
        return [c for c in df.columns if c not in exclude_cols and df[c].dtype in ['float64', 'int64', 'float32', 'int32']]
    
    def run_backtest(self, sector_history: pd.DataFrame, 
                     limit_up_history: pd.DataFrame,
                     test_start_date: str,
                     test_end_date: str) -> pd.DataFrame:
        """
        è¿è¡Œæ»šåŠ¨å›æµ‹
        
        Args:
            sector_history: æ¿å—å†å²è¡Œæƒ…
            limit_up_history: æ¶¨åœæ± å†å²
            test_start_date: æµ‹è¯•å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            test_end_date: æµ‹è¯•ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            
        Returns:
            å›æµ‹ç»“æœDataFrame
        """
        logger.info(f"å¼€å§‹æ»šåŠ¨å›æµ‹: {test_start_date} -> {test_end_date}")
        logger.info(f"è®­ç»ƒçª—å£: {self.train_window_months}ä¸ªæœˆ, æ­¥é•¿: {self.step_months}ä¸ªæœˆ")
        
        # èšåˆæ¶¨åœæ•°æ®
        from .historical_data import HistoricalDataFetcher
        fetcher = HistoricalDataFetcher()
        limit_up_agg = fetcher.aggregate_limit_up_by_sector(limit_up_history)
        
        # åˆ›å»ºç‰¹å¾
        logger.info("åˆ›å»ºç‰¹å¾æ•°æ®é›†...")
        df = self.feature_engineer.create_features(sector_history, limit_up_agg)
        
        if df.empty:
            logger.error("ç‰¹å¾æ•°æ®ä¸ºç©º!")
            return pd.DataFrame()
        
        # è·å–ç‰¹å¾åˆ—
        feature_cols = self._get_feature_columns(df)
        logger.info(f"ä½¿ç”¨ç‰¹å¾: {len(feature_cols)} ä¸ª")
        
        # è·å–æµ‹è¯•æœŸå†…çš„æ‰€æœ‰æ—¥æœŸ
        test_dates = df[(df['date'] >= test_start_date) & 
                        (df['date'] <= test_end_date)]['date'].unique()
        test_dates = sorted(test_dates)
        
        logger.info(f"æµ‹è¯•æ—¥æœŸ: {len(test_dates)} å¤©")
        
        all_predictions = []
        
        # æŒ‰æœˆæ»šåŠ¨
        current_month = None
        model = None
        
        for i, test_date in enumerate(test_dates):
            test_month = test_date[:7]  # YYYY-MM
            
            # æ¯æœˆé‡æ–°è®­ç»ƒæ¨¡å‹
            if test_month != current_month:
                current_month = test_month
                
                # è®¡ç®—è®­ç»ƒçª—å£
                train_end = datetime.strptime(test_date, '%Y-%m-%d') - timedelta(days=1)
                train_start = train_end - timedelta(days=self.train_window_months * 30)
                
                train_end_str = train_end.strftime('%Y-%m-%d')
                train_start_str = train_start.strftime('%Y-%m-%d')
                
                # è·å–è®­ç»ƒæ•°æ®
                train_df = df[(df['date'] >= train_start_str) & 
                              (df['date'] <= train_end_str)]
                
                if len(train_df) < 100:
                    logger.warning(f"è®­ç»ƒæ•°æ®ä¸è¶³: {len(train_df)} æ¡, è·³è¿‡ {test_month}")
                    continue
                
                # è®­ç»ƒæ¨¡å‹
                X_train = train_df[feature_cols].fillna(0)
                y_train = train_df['label_score']
                
                train_data = lgb.Dataset(X_train, label=y_train)
                
                model = lgb.train(
                    self.lgbm_params,
                    train_data,
                    num_boost_round=300,
                    callbacks=[lgb.log_evaluation(period=0)]
                )
                
                logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆ ({test_month}): {len(train_df)} æ ·æœ¬, "
                           f"{train_start_str} -> {train_end_str}")
            
            if model is None:
                continue
            
            # è·å–æµ‹è¯•æ—¥çš„æ•°æ®
            test_df = df[df['date'] == test_date].copy()
            
            if test_df.empty:
                continue
            
            # é¢„æµ‹
            X_test = test_df[feature_cols].fillna(0)
            test_df['pred_score'] = model.predict(X_test)
            
            # æŒ‰é¢„æµ‹åˆ†æ•°æ’å
            test_df['pred_rank'] = test_df['pred_score'].rank(ascending=False)
            
            # ä¿å­˜ç»“æœ
            result = test_df[['date', 'sector_name', 'sector_type',
                              'change_pct', 'limit_up_count',
                              'label_change_pct', 'label_limit_up_count', 'label_score',
                              'pred_score', 'pred_rank']].copy()
            all_predictions.append(result)
            
            if (i + 1) % 50 == 0:
                logger.info(f"å›æµ‹è¿›åº¦: {i+1}/{len(test_dates)} ({(i+1)/len(test_dates)*100:.1f}%)")
        
        if not all_predictions:
            logger.error("æ²¡æœ‰ç”Ÿæˆä»»ä½•é¢„æµ‹ç»“æœ!")
            return pd.DataFrame()
        
        results = pd.concat(all_predictions, ignore_index=True)
        self.results = results
        
        logger.info(f"å›æµ‹å®Œæˆ: {len(results)} æ¡é¢„æµ‹, {results['date'].nunique()} ä¸ªäº¤æ˜“æ—¥")
        
        return results
    
    def save_results(self, results: pd.DataFrame, name: str = "backtest"):
        """ä¿å­˜å›æµ‹ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        path = BACKTEST_RESULTS_DIR / f"{name}_{timestamp}.parquet"
        results.to_parquet(path, index=False)
        logger.info(f"å›æµ‹ç»“æœå·²ä¿å­˜: {path}")
        return path


class BacktestEvaluator:
    """å›æµ‹ç»“æœè¯„ä¼°å™¨"""
    
    def __init__(self, results: pd.DataFrame):
        """
        Args:
            results: å›æµ‹ç»“æœDataFrame
        """
        self.results = results
        self.daily_metrics = None
        self.overall_metrics = None
    
    def calculate_daily_metrics(self, top_k: int = 5) -> pd.DataFrame:
        """
        è®¡ç®—æ¯æ—¥è¯„ä¼°æŒ‡æ ‡
        
        Args:
            top_k: é€‰å–Top-Kä¸ªé¢„æµ‹
            
        Returns:
            æ¯æ—¥æŒ‡æ ‡DataFrame
        """
        daily_metrics = []
        
        for date in self.results['date'].unique():
            day_df = self.results[self.results['date'] == date].copy()
            
            if len(day_df) < top_k:
                continue
            
            # é€‰å–é¢„æµ‹æ’åå‰Kçš„æ¿å—
            top_k_df = day_df.nsmallest(top_k, 'pred_rank')
            
            # è®¡ç®—æŒ‡æ ‡
            top_k_avg_return = top_k_df['label_change_pct'].mean()
            market_avg_return = day_df['label_change_pct'].mean()
            excess_return = top_k_avg_return - market_avg_return
            
            # è®¡ç®—é¢„æµ‹æ¿å—åœ¨å…¨å¸‚åœºçš„æ’åè¡¨ç°
            # å¦‚æœé¢„æµ‹çš„Top-Kæ¿å—å®é™…æ¶¨å¹…æ’åœ¨å‰20%ï¼Œç®—å‘½ä¸­
            day_df_sorted = day_df.sort_values('label_change_pct', ascending=False)
            top_20pct_threshold = day_df_sorted['label_change_pct'].quantile(0.8)
            top_10pct_threshold = day_df_sorted['label_change_pct'].quantile(0.9)
            
            metrics = {
                'date': date,
                'total_sectors': len(day_df),
                
                # æ”¶ç›ŠæŒ‡æ ‡
                'top_k_avg_return': top_k_avg_return,
                'top_k_total_return': top_k_df['label_change_pct'].sum(),
                'market_avg_return': market_avg_return,
                'excess_return': excess_return,
                
                # å‘½ä¸­ç‡æŒ‡æ ‡ï¼ˆåŸºäºæ¿å—æ¶¨å¹…ï¼‰
                # æ¶¨å¹…>1%å‘½ä¸­ç‡
                'hit_rate_1pct': (top_k_df['label_change_pct'] > 1).mean(),
                # æ¶¨å¹…>2%å‘½ä¸­ç‡
                'hit_rate_2pct': (top_k_df['label_change_pct'] > 2).mean(),
                # æ¶¨å¹…>3%å‘½ä¸­ç‡ï¼ˆå¼ºåŠ¿æ¿å—ï¼‰
                'hit_rate_3pct': (top_k_df['label_change_pct'] > 3).mean(),
                
                # æ’åå‘½ä¸­ç‡
                # é¢„æµ‹æ¿å—è¿›å…¥å½“æ—¥æ¶¨å¹…å‰20%
                'rank_hit_top20': (top_k_df['label_change_pct'] >= top_20pct_threshold).mean(),
                # é¢„æµ‹æ¿å—è¿›å…¥å½“æ—¥æ¶¨å¹…å‰10%
                'rank_hit_top10': (top_k_df['label_change_pct'] >= top_10pct_threshold).mean(),
                
                # è¶…é¢æ”¶ç›Šå‘½ä¸­
                # é¢„æµ‹æ¿å—æ¶¨å¹…è¶…è¿‡å¸‚åœºå¹³å‡
                'beat_market_rate': (top_k_df['label_change_pct'] > market_avg_return).mean(),
            }
            
            # è®¡ç®—NDCG@K
            if len(day_df) >= top_k:
                try:
                    true_relevance = day_df['label_score'].values.reshape(1, -1)
                    pred_scores = day_df['pred_score'].values.reshape(1, -1)
                    metrics['ndcg'] = ndcg_score(true_relevance, pred_scores, k=top_k)
                except:
                    metrics['ndcg'] = np.nan
            else:
                metrics['ndcg'] = np.nan
            
            daily_metrics.append(metrics)
        
        self.daily_metrics = pd.DataFrame(daily_metrics)
        return self.daily_metrics
    
    def calculate_overall_metrics(self) -> Dict:
        """è®¡ç®—æ•´ä½“è¯„ä¼°æŒ‡æ ‡"""
        if self.daily_metrics is None:
            self.calculate_daily_metrics()
        
        dm = self.daily_metrics
        
        # åŸºç¡€ç»Ÿè®¡
        total_days = len(dm)
        
        # æ”¶ç›Šç»Ÿè®¡
        avg_daily_return = dm['top_k_avg_return'].mean()
        total_return = dm['top_k_avg_return'].sum()
        avg_excess_return = dm['excess_return'].mean()
        total_excess_return = dm['excess_return'].sum()
        
        # èƒœç‡ç»Ÿè®¡ï¼ˆé¢„æµ‹æ¿å—æ”¶ç›Š>0ï¼‰
        win_days = (dm['top_k_avg_return'] > 0).sum()
        win_rate = win_days / total_days if total_days > 0 else 0
        
        # è¶…é¢èƒœç‡ï¼ˆé¢„æµ‹æ¿å—è·‘èµ¢å¤§ç›˜çš„å¤©æ•°ï¼‰
        beat_market_days = (dm['excess_return'] > 0).sum()
        beat_market_rate = beat_market_days / total_days if total_days > 0 else 0
        
        # å‘½ä¸­ç‡ç»Ÿè®¡ï¼ˆåŸºäºæ¶¨å¹…é˜ˆå€¼ï¼‰
        avg_hit_rate_1pct = dm['hit_rate_1pct'].mean()
        avg_hit_rate_2pct = dm['hit_rate_2pct'].mean()
        avg_hit_rate_3pct = dm['hit_rate_3pct'].mean()
        
        # æ’åå‘½ä¸­ç‡
        avg_rank_hit_top20 = dm['rank_hit_top20'].mean()
        avg_rank_hit_top10 = dm['rank_hit_top10'].mean()
        avg_beat_market = dm['beat_market_rate'].mean()
        
        # NDCGç»Ÿè®¡
        avg_ndcg = dm['ndcg'].mean()
        
        # é£é™©æŒ‡æ ‡
        daily_returns = dm['top_k_avg_return']
        sharpe_ratio = (daily_returns.mean() / daily_returns.std() * np.sqrt(252)) if daily_returns.std() > 0 else 0
        
        # æœ€å¤§å›æ’¤
        cumulative_return = (1 + daily_returns / 100).cumprod()
        rolling_max = cumulative_return.expanding().max()
        drawdown = (cumulative_return - rolling_max) / rolling_max
        max_drawdown = drawdown.min() * 100
        
        # æŒ‰æˆåŠŸ/å¤±è´¥å¤©æ•°ç»Ÿè®¡ï¼ˆä»¥è·‘èµ¢å¸‚åœºä¸ºæ ‡å‡†ï¼‰
        success_days = dm[dm['excess_return'] > 0]
        fail_days = dm[dm['excess_return'] <= 0]
        
        avg_success_return = success_days['top_k_avg_return'].mean() if len(success_days) > 0 else 0
        avg_fail_return = fail_days['top_k_avg_return'].mean() if len(fail_days) > 0 else 0
        
        self.overall_metrics = {
            # åŸºç¡€ç»Ÿè®¡
            'total_trading_days': total_days,
            'win_days': win_days,
            'beat_market_days': beat_market_days,
            'success_days': len(success_days),
            'fail_days': len(fail_days),
            'win_rate': win_rate,
            'beat_market_rate': beat_market_rate,
            
            # æ”¶ç›ŠæŒ‡æ ‡
            'avg_daily_return': avg_daily_return,
            'total_return': total_return,
            'avg_excess_return': avg_excess_return,
            'total_excess_return': total_excess_return,
            'avg_success_return': avg_success_return,
            'avg_fail_return': avg_fail_return,
            
            # å‘½ä¸­ç‡æŒ‡æ ‡ï¼ˆæ¿å—æ¶¨å¹…é˜ˆå€¼ï¼‰
            'hit_rate_1pct': avg_hit_rate_1pct,
            'hit_rate_2pct': avg_hit_rate_2pct,
            'hit_rate_3pct': avg_hit_rate_3pct,
            
            # æ’åå‘½ä¸­ç‡
            'rank_hit_top20': avg_rank_hit_top20,
            'rank_hit_top10': avg_rank_hit_top10,
            'beat_market_avg': avg_beat_market,
            
            # æ’åºæŒ‡æ ‡
            'avg_ndcg': avg_ndcg,
            
            # é£é™©æŒ‡æ ‡
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'return_std': daily_returns.std(),
        }
        
        return self.overall_metrics
    
    def generate_report(self, output_path: str = None) -> str:
        """ç”Ÿæˆå›æµ‹æŠ¥å‘Š"""
        if self.overall_metrics is None:
            self.calculate_overall_metrics()
        
        om = self.overall_metrics
        dm = self.daily_metrics
        
        # æœˆåº¦ç»Ÿè®¡
        dm['month'] = pd.to_datetime(dm['date']).dt.strftime('%Y-%m')
        monthly = dm.groupby('month').agg({
            'top_k_avg_return': 'sum',
            'excess_return': 'sum',
            'rank_hit_top20': 'mean',
            'ndcg': 'mean',
        }).reset_index()
        
        report_lines = [
            "# ğŸ“Š Aè‚¡æ¿å—é¢„æµ‹ç³»ç»Ÿ - å›æµ‹éªŒè¯æŠ¥å‘Š",
            "",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "---",
            "",
            "## ğŸ“ˆ æ•´ä½“ç»©æ•ˆç»Ÿè®¡",
            "",
            "### åŸºç¡€ç»Ÿè®¡",
            "",
            f"| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |",
            f"|------|------|------|",
            f"| æ€»äº¤æ˜“å¤©æ•° | {om['total_trading_days']} | å›æµ‹è¦†ç›–çš„äº¤æ˜“æ—¥ |",
            f"| ç›ˆåˆ©å¤©æ•° | {om['win_days']} | é¢„æµ‹æ¿å—æ”¶ç›Š>0çš„å¤©æ•° |",
            f"| è·‘èµ¢å¸‚åœºå¤©æ•° | {om['beat_market_days']} | é¢„æµ‹æ¿å—è·‘èµ¢å¤§ç›˜çš„å¤©æ•° |",
            f"| **èƒœç‡** | **{om['win_rate']:.2%}** | ç›ˆåˆ©å¤©æ•°/æ€»å¤©æ•° |",
            f"| **è¶…é¢èƒœç‡** | **{om['beat_market_rate']:.2%}** | è·‘èµ¢å¸‚åœºå¤©æ•°/æ€»å¤©æ•° |",
            "",
            "### æ”¶ç›ŠæŒ‡æ ‡",
            "",
            f"| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |",
            f"|------|------|------|",
            f"| æ—¥å‡æ”¶ç›Š | {om['avg_daily_return']:.2f}% | é¢„æµ‹æ¿å—çš„å¹³å‡æ—¥æ”¶ç›Š |",
            f"| **ç´¯è®¡æ”¶ç›Š** | **{om['total_return']:.2f}%** | å…¨å‘¨æœŸç´¯è®¡æ”¶ç›Š |",
            f"| æ—¥å‡è¶…é¢æ”¶ç›Š | {om['avg_excess_return']:.2f}% | è¶…è¿‡å¸‚åœºå¹³å‡çš„æ”¶ç›Š |",
            f"| **ç´¯è®¡è¶…é¢æ”¶ç›Š** | **{om['total_excess_return']:.2f}%** | å…¨å‘¨æœŸç´¯è®¡è¶…é¢ |",
            f"| æˆåŠŸæ—¥å¹³å‡æ¶¨å¹… | {om['avg_success_return']:.2f}% | è·‘èµ¢å¸‚åœºæ—¶çš„å¹³å‡æ”¶ç›Š |",
            f"| å¤±è´¥æ—¥å¹³å‡æ”¶ç›Š | {om['avg_fail_return']:.2f}% | è·‘è¾“å¸‚åœºæ—¶çš„å¹³å‡æ”¶ç›Š |",
            "",
            "### å‘½ä¸­ç‡æŒ‡æ ‡ï¼ˆæ¿å—æ¶¨å¹…ï¼‰",
            "",
            f"| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |",
            f"|------|------|------|",
            f"| æ¶¨å¹…>1%å‘½ä¸­ç‡ | {om['hit_rate_1pct']:.2%} | é¢„æµ‹æ¿å—æ¶¨å¹…è¶…1%çš„æ¯”ä¾‹ |",
            f"| æ¶¨å¹…>2%å‘½ä¸­ç‡ | {om['hit_rate_2pct']:.2%} | é¢„æµ‹æ¿å—æ¶¨å¹…è¶…2%çš„æ¯”ä¾‹ |",
            f"| **æ¶¨å¹…>3%å‘½ä¸­ç‡** | **{om['hit_rate_3pct']:.2%}** | é¢„æµ‹æ¿å—æ¶¨å¹…è¶…3%çš„æ¯”ä¾‹ |",
            "",
            "### æ’åå‘½ä¸­ç‡",
            "",
            f"| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |",
            f"|------|------|------|",
            f"| **è¿›å…¥Top20%** | **{om['rank_hit_top20']:.2%}** | é¢„æµ‹æ¿å—å®é™…æ¶¨å¹…æ’å‰20% |",
            f"| è¿›å…¥Top10% | {om['rank_hit_top10']:.2%} | é¢„æµ‹æ¿å—å®é™…æ¶¨å¹…æ’å‰10% |",
            f"| è·‘èµ¢å¸‚åœºå‡å€¼ | {om['beat_market_avg']:.2%} | é¢„æµ‹æ¿å—æ¶¨å¹…è¶…è¿‡å¸‚åœºå‡å€¼ |",
            "",
            "### æ’åºè´¨é‡ä¸é£é™©æŒ‡æ ‡",
            "",
            f"| æŒ‡æ ‡ | æ•°å€¼ | è¯´æ˜ |",
            f"|------|------|------|",
            f"| **NDCG@5** | **{om['avg_ndcg']:.4f}** | æ’åºè´¨é‡è¯„åˆ†(0-1) |",
            f"| **å¤æ™®æ¯”ç‡** | **{om['sharpe_ratio']:.2f}** | é£é™©è°ƒæ•´åæ”¶ç›Š |",
            f"| æœ€å¤§å›æ’¤ | {om['max_drawdown']:.2f}% | æœ€å¤§äºæŸå¹…åº¦ |",
            f"| æ—¥æ”¶ç›Šæ ‡å‡†å·® | {om['return_std']:.2f}% | æ”¶ç›Šæ³¢åŠ¨ç‡ |",
            "",
            "---",
            "",
            "## ğŸ“… æœˆåº¦ç»©æ•ˆ",
            "",
            "| æœˆä»½ | ç´¯è®¡æ”¶ç›Š | è¶…é¢æ”¶ç›Š | Top20%å‘½ä¸­ | NDCG |",
            "|------|---------|---------|------------|------|",
        ]
        
        for _, row in monthly.iterrows():
            report_lines.append(
                f"| {row['month']} | {row['top_k_avg_return']:.2f}% | "
                f"{row['excess_return']:.2f}% | {row['rank_hit_top20']:.2%} | "
                f"{row['ndcg']:.4f} |"
            )
        
        report_lines.extend([
            "",
            "---",
            "",
            "## ğŸ’¡ ç»“è®ºä¸å»ºè®®",
            "",
        ])
        
        # æ ¹æ®ç»“æœç»™å‡ºè¯„ä»·
        if om['avg_ndcg'] > 0.65 and om['beat_market_rate'] > 0.55:
            report_lines.append("âœ… **æ¨¡å‹è¡¨ç°ä¼˜ç§€**: NDCGå’Œè¶…é¢èƒœç‡å‡è¾¾åˆ°è¾ƒé«˜æ°´å¹³ï¼Œå¯è€ƒè™‘å®ç›˜æµ‹è¯•ã€‚")
        elif om['avg_ndcg'] > 0.55 and om['beat_market_rate'] > 0.50:
            report_lines.append("ğŸŸ¡ **æ¨¡å‹è¡¨ç°è‰¯å¥½**: æœ‰ä¸€å®šé¢„æµ‹èƒ½åŠ›ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–ç‰¹å¾å’Œå‚æ•°ã€‚")
        else:
            report_lines.append("ğŸ”´ **æ¨¡å‹éœ€è¦æ”¹è¿›**: é¢„æµ‹æ•ˆæœä¸ç†æƒ³ï¼Œéœ€è¦é‡æ–°å®¡è§†ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹é€‰æ‹©ã€‚")
        
        report_lines.extend([
            "",
            "---",
            "",
            "*æŠ¥å‘Šç”± Aè‚¡æ¿å—æ¶¨åœé¢„æµ‹ç³»ç»Ÿ è‡ªåŠ¨ç”Ÿæˆ*",
        ])
        
        report_content = "\n".join(report_lines)
        
        if output_path is None:
            output_path = BACKTEST_RESULTS_DIR / f"backtest_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"å›æµ‹æŠ¥å‘Šå·²ç”Ÿæˆ: {output_path}")
        return str(output_path)
    
    def print_summary(self):
        """æ‰“å°å›æµ‹æ‘˜è¦"""
        if self.overall_metrics is None:
            self.calculate_overall_metrics()
        
        om = self.overall_metrics
        
        print("\n" + "=" * 60)
        print("ğŸ“Š å›æµ‹ç»©æ•ˆæ‘˜è¦")
        print("=" * 60)
        
        print(f"\nğŸ“… äº¤æ˜“ç»Ÿè®¡:")
        print(f"   æ€»äº¤æ˜“å¤©æ•°: {om['total_trading_days']}")
        print(f"   ç›ˆåˆ©å¤©æ•°: {om['win_days']} ({om['win_rate']*100:.1f}%)")
        print(f"   è·‘èµ¢å¸‚åœºå¤©æ•°: {om['beat_market_days']} ({om['beat_market_rate']*100:.1f}%)")
        
        print(f"\nğŸ’° æ”¶ç›Šç»Ÿè®¡:")
        print(f"   æ—¥å‡æ”¶ç›Š: {om['avg_daily_return']:.2f}%")
        print(f"   ç´¯è®¡æ”¶ç›Š: {om['total_return']:.2f}%")
        print(f"   æ—¥å‡è¶…é¢: {om['avg_excess_return']:.2f}%")
        print(f"   ç´¯è®¡è¶…é¢: {om['total_excess_return']:.2f}%")
        
        print(f"\nğŸ¯ å‘½ä¸­ç‡ï¼ˆæ¿å—æ¶¨å¹…ï¼‰:")
        print(f"   æ¶¨å¹…>1%å‘½ä¸­ç‡: {om['hit_rate_1pct']:.2%}")
        print(f"   æ¶¨å¹…>2%å‘½ä¸­ç‡: {om['hit_rate_2pct']:.2%}")
        print(f"   æ¶¨å¹…>3%å‘½ä¸­ç‡: {om['hit_rate_3pct']:.2%}")
        
        print(f"\nğŸ“Š æ’åå‘½ä¸­ç‡:")
        print(f"   è¿›å…¥Top20%: {om['rank_hit_top20']:.2%}")
        print(f"   è¿›å…¥Top10%: {om['rank_hit_top10']:.2%}")
        print(f"   è·‘èµ¢å¸‚åœºå‡å€¼: {om['beat_market_avg']:.2%}")
        
        print(f"\nğŸ“ˆ æ’åºè´¨é‡:")
        print(f"   NDCG@5: {om['avg_ndcg']:.4f}")
        
        print(f"\nâš ï¸ é£é™©æŒ‡æ ‡:")
        print(f"   å¤æ™®æ¯”ç‡: {om['sharpe_ratio']:.2f}")
        print(f"   æœ€å¤§å›æ’¤: {om['max_drawdown']:.2f}%")
        
        print("\n" + "=" * 60)


def run_full_backtest(train_years: Tuple[int, int] = (2022, 2023),
                      test_year: int = 2024):
    """
    è¿è¡Œå®Œæ•´å›æµ‹æµç¨‹
    
    Args:
        train_years: è®­ç»ƒå¹´ä»½èŒƒå›´
        test_year: æµ‹è¯•å¹´ä»½
    """
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s"
    )
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹å†å²å›æµ‹éªŒè¯")
    print(f"   è®­ç»ƒæ•°æ®: {train_years[0]} - {train_years[1]}")
    print(f"   æµ‹è¯•æ•°æ®: {test_year}")
    print(f"{'='*60}\n")
    
    # 1. åŠ è½½å†å²æ•°æ®
    from .historical_data import HistoricalDataFetcher
    fetcher = HistoricalDataFetcher()
    
    print("ğŸ“‚ åŠ è½½å†å²æ•°æ®...")
    sector_history, limit_up_history = fetcher.load_historical_data()
    
    if sector_history.empty:
        print("âŒ å†å²æ•°æ®ä¸ºç©º! è¯·å…ˆè¿è¡Œ:")
        print("   python -m backtest.historical_data")
        return
    
    print(f"   âœ“ æ¿å—æ•°æ®: {len(sector_history)} æ¡")
    print(f"   âœ“ æ¶¨åœæ•°æ®: {len(limit_up_history)} æ¡\n")
    
    # 2. è¿è¡Œå›æµ‹
    print("ğŸ”„ è¿è¡Œæ»šåŠ¨å›æµ‹...")
    engine = RollingBacktestEngine(
        train_window_months=24,
        valid_window_months=3,
        step_months=1
    )
    
    results = engine.run_backtest(
        sector_history=sector_history,
        limit_up_history=limit_up_history,
        test_start_date=f"{test_year}-01-01",
        test_end_date=f"{test_year}-12-31"
    )
    
    if results.empty:
        print("âŒ å›æµ‹æ²¡æœ‰ç”Ÿæˆç»“æœ!")
        return
    
    # 3. ä¿å­˜ç»“æœ
    engine.save_results(results, f"backtest_{train_years[0]}-{train_years[1]}_test_{test_year}")
    
    # 4. è¯„ä¼°ç»“æœ
    print("\nğŸ“Š è¯„ä¼°å›æµ‹ç»“æœ...")
    evaluator = BacktestEvaluator(results)
    evaluator.calculate_daily_metrics(top_k=5)
    evaluator.calculate_overall_metrics()
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    report_path = evaluator.generate_report()
    
    # 6. æ‰“å°æ‘˜è¦
    evaluator.print_summary()
    
    print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_path}")
    print(f"{'='*60}\n")
    
    return evaluator


if __name__ == "__main__":
    # è¿è¡Œ 2022-2023 è®­ç»ƒ -> 2024 æµ‹è¯• çš„å›æµ‹
    run_full_backtest(train_years=(2022, 2023), test_year=2024)
