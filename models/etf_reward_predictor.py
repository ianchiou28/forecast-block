"""
Aè‚¡ETFé¢„æµ‹ç³»ç»Ÿ - å¸¦å¥–æƒ©æœºåˆ¶çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹
ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ€æƒ³ï¼šåŸºäºå®é™…æ”¶ç›Šè¿›è¡Œå¥–åŠ±/æƒ©ç½šè®­ç»ƒ
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Optional, Dict, List, Tuple
import logging
import pickle
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, ndcg_score

from config.settings import MODEL_CONFIG, MODEL_DIR

logger = logging.getLogger(__name__)

# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(42)


class RewardETFNet(nn.Module):
    """å¸¦å¥–æƒ©æœºåˆ¶çš„ETFé¢„æµ‹æ·±åº¦ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout_rate=0.3):
        super(RewardETFNet, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        
        self.feature_layers = nn.Sequential(*layers)
        self.output = nn.Linear(prev_dim, 1)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, x):
        features = self.feature_layers(x)
        output = self.output(features)
        return output


class RewardLoss(nn.Module):
    """
    è‡ªå®šä¹‰å¥–æƒ©æŸå¤±å‡½æ•° - è½»æƒ©ç½šé‡å¥–åŠ±ç‰ˆæœ¬
    
    æ ¸å¿ƒæ´å¯Ÿï¼š
    1. é¢„æµ‹ Top 3ï¼Œå®é™…ä¸‹è·Œ â†’ æ™®é€šæƒ©ç½š
    2. é¢„æµ‹ Top 3ï¼Œå®é™…å¤§æ¶¨ â†’ åŠ é‡å¥–åŠ±ï¼ˆè¿™æ˜¯æˆ‘ä»¬è¿½æ±‚çš„ç›®æ ‡ï¼‰
    
    å…³é”®å‚æ•°ï¼š
    - top_k_strict=3: ä¸¥æ ¼çš„ Top 3 åˆ¤æ–­
    - big_gain_threshold=2.0: å¤§æ¶¨é˜ˆå€¼ (2%)
    - big_gain_reward_multiplier=2.0: å¤§æ¶¨å¥–åŠ±å€æ•°
    """
    
    def __init__(self, reward_weight=1.0, penalty_weight=0.5, top_k=5, 
                 top_k_strict=3, big_gain_threshold=2.0,
                 big_gain_reward_multiplier=2.0):
        super(RewardLoss, self).__init__()
        self.mse = nn.MSELoss(reduction='none')
        self.reward_weight = reward_weight  # åŸºç¡€å¥–åŠ±æƒé‡
        self.penalty_weight = penalty_weight  # åŸºç¡€æƒ©ç½šæƒé‡
        self.top_k = top_k
        self.top_k_strict = top_k_strict  # ä¸¥æ ¼çš„ Top K (ç”¨äºåŠ é‡å¥–åŠ±åˆ¤æ–­)
        self.big_gain_threshold = big_gain_threshold  # å¤§æ¶¨é˜ˆå€¼ (%)
        self.big_gain_reward_multiplier = big_gain_reward_multiplier  # å¤§æ¶¨å¥–åŠ±å€æ•°
    
    def forward(self, pred_scores, true_scores, actual_returns=None):
        """
        è®¡ç®—å¥–æƒ©æŸå¤±
        
        Args:
            pred_scores: é¢„æµ‹åˆ†æ•° (batch_size, 1)
            true_scores: çœŸå®æ’ååˆ†æ•° (batch_size, 1)
            actual_returns: å®é™…æ”¶ç›Šç‡ (batch_size, 1)ï¼Œç”¨äºå¥–æƒ©
        """
        # åŸºç¡€MSEæŸå¤±
        base_loss = self.mse(pred_scores, true_scores)
        
        if actual_returns is None:
            return base_loss.mean()
        
        # è®¡ç®—é¢„æµ‹æ’å
        pred_ranks = self._get_ranks(pred_scores)
        
        # å¥–æƒ©è°ƒæ•´
        batch_size = pred_scores.size(0)
        adjustments = torch.zeros_like(base_loss)
        
        for i in range(batch_size):
            pred_rank = pred_ranks[i].item()
            actual_ret = actual_returns[i].item()
            
            # === æ ¸å¿ƒé€»è¾‘ï¼šè½»æƒ©ç½šã€é‡å¥–åŠ± ===
            
            if pred_rank <= self.top_k_strict:
                # é¢„æµ‹ä¸º Top 3 (ä¸¥æ ¼åˆ¤æ–­)
                if actual_ret < 0:
                    # é¢„æµ‹ Top 3 ä½†å®é™…ä¸‹è·Œ â†’ æ™®é€šæƒ©ç½š
                    penalty = self.penalty_weight * abs(actual_ret) / 100
                    adjustments[i] = penalty
                elif actual_ret >= self.big_gain_threshold:
                    # ğŸŸ¢ å®Œç¾é¢„æµ‹ï¼šé¢„æµ‹ Top 3 ä¸”å¤§æ¶¨ â†’ åŠ é‡å¥–åŠ±
                    reward = self.reward_weight * self.big_gain_reward_multiplier * actual_ret / 100
                    adjustments[i] = -reward
                elif actual_ret > 0:
                    # é¢„æµ‹ Top 3 ä¸”å°æ¶¨ â†’ æ™®é€šå¥–åŠ±
                    reward = self.reward_weight * actual_ret / 100
                    adjustments[i] = -reward
                    
            elif pred_rank <= self.top_k:
                # é¢„æµ‹ä¸º Top 4-5
                if actual_ret < 0:
                    # é¢„æµ‹é å‰ä½†ä¸‹è·Œ â†’ è½»æƒ©ç½š
                    penalty = self.penalty_weight * 0.5 * abs(actual_ret) / 100
                    adjustments[i] = penalty
                elif actual_ret >= self.big_gain_threshold:
                    # é¢„æµ‹é å‰ä¸”å¤§æ¶¨ â†’ å¥–åŠ±
                    reward = self.reward_weight * actual_ret / 100
                    adjustments[i] = -reward
                elif actual_ret > 0:
                    # å°æ¶¨ â†’ å°å¥–åŠ±
                    reward = self.reward_weight * 0.5 * actual_ret / 100
                    adjustments[i] = -reward
            else:
                # é¢„æµ‹ä¸ºé Top-K
                if actual_ret >= self.big_gain_threshold:
                    # é”™è¿‡å¤§æ¶¨ â†’ è½»æƒ©ç½š
                    penalty = self.penalty_weight * 0.3 * abs(actual_ret) / 100
                    adjustments[i] = penalty
                # é¢„æµ‹é Top-K ä¸”ç¡®å®æ²¡æ¶¨ â†’ æ­£ç¡®ï¼Œæ— è°ƒæ•´
        
        # ç»„åˆæŸå¤±
        total_loss = base_loss + adjustments
        return total_loss.mean()
    
    def _get_ranks(self, scores):
        """è·å–æ’åï¼ˆ1ä¸ºæœ€é«˜ï¼‰"""
        sorted_indices = torch.argsort(scores.squeeze(), descending=True)
        ranks = torch.zeros_like(sorted_indices)
        ranks[sorted_indices] = torch.arange(1, len(scores) + 1, device=scores.device)
        return ranks


class ETFRewardModel:
    """å¸¦å¥–æƒ©æœºåˆ¶çš„ETFé¢„æµ‹æ¨¡å‹"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ¨¡å‹ä¿å­˜è·¯å¾„
        self.model_path = MODEL_DIR / "etf_reward_model.pth"
        self.scaler_path = MODEL_DIR / "etf_reward_scaler.pkl"
        self.history_path = MODEL_DIR / "etf_reward_training_history.json"
        
        # ç‰¹å¾åˆ—
        self.feature_columns = self._get_feature_columns()
        
        logger.info(f"å¥–æƒ©æ¨¡å‹ä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def _get_feature_columns(self) -> List[str]:
        """è·å–ç‰¹å¾åˆ—å"""
        features = []
        
        # åŠ¨é‡ç‰¹å¾
        for window in [3, 5, 10, 20]:
            features.extend([f"return_{window}d", f"volatility_{window}d"])
        
        # æˆäº¤é‡ç‰¹å¾
        features.extend(["volume_ratio", "turnover_ratio"])
        
        # ä»·æ ¼ä½ç½®
        features.append("price_position")
        
        # å‡çº¿åç¦»
        features.extend(["ma5_bias", "ma10_bias", "ma20_bias"])
        
        # èµ„é‡‘æµ
        features.extend(["money_flow_ma3", "money_flow_ma5", "money_flow_momentum"])
        
        # æŠ€æœ¯æŒ‡æ ‡
        features.extend(["rsi_14", "atr_14"])
        
        return features
    
    def train_with_reward(self, df: pd.DataFrame,
                          train_start: str = None,
                          train_end: str = None,
                          epochs: int = 150,
                          batch_size: int = 64,
                          learning_rate: float = 0.001,
                          reward_weight: float = 1.0,
                          penalty_weight: float = 0.5,
                          top_k_strict: int = 3,
                          big_gain_threshold: float = 2.0,
                          big_gain_reward_multiplier: float = 2.0) -> Dict:
        """
        ä½¿ç”¨å¥–æƒ©æœºåˆ¶è®­ç»ƒæ¨¡å‹ - è½»æƒ©ç½šé‡å¥–åŠ±ç‰ˆæœ¬
        
        æ ¸å¿ƒç­–ç•¥ï¼š
        - é¢„æµ‹ Top 3 ä¸”å¤§æ¶¨ â†’ åŠ é‡å¥–åŠ±ï¼ˆé‡ç‚¹é¼“åŠ±ï¼‰
        - é¢„æµ‹ Top 3 ä½†ä¸‹è·Œ â†’ æ™®é€šæƒ©ç½š
        
        Args:
            df: åŒ…å«ç‰¹å¾å’Œæ ‡ç­¾çš„æ•°æ®
            train_start/train_end: è®­ç»ƒæ•°æ®èŒƒå›´
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤§å°
            learning_rate: å­¦ä¹ ç‡
            reward_weight: åŸºç¡€å¥–åŠ±æƒé‡ (é»˜è®¤1.0)
            penalty_weight: åŸºç¡€æƒ©ç½šæƒé‡ (é»˜è®¤0.5)
            top_k_strict: ä¸¥æ ¼ Top-K åˆ¤æ–­é˜ˆå€¼ (é»˜è®¤3)
            big_gain_threshold: å¤§æ¶¨é˜ˆå€¼ç™¾åˆ†æ¯” (é»˜è®¤2.0%)
            big_gain_reward_multiplier: å¤§æ¶¨åŠ é‡å¥–åŠ±å€æ•° (é»˜è®¤2.0)
        """
        logger.info("=" * 60)
        logger.info("ğŸ¯ å¼€å§‹ã€è½»æƒ©ç½šé‡å¥–åŠ±ã€‘æ·±åº¦å­¦ä¹ è®­ç»ƒ...")
        logger.info(f"   åŸºç¡€å¥–åŠ±æƒé‡: {reward_weight}, åŸºç¡€æƒ©ç½šæƒé‡: {penalty_weight}")
        logger.info(f"   ä¸¥æ ¼ Top-K: {top_k_strict}, å¤§æ¶¨é˜ˆå€¼: {big_gain_threshold}%")
        logger.info(f"   å¤§æ¶¨åŠ é‡å¥–åŠ±å€æ•°: {big_gain_reward_multiplier}x")
        
        # æ•°æ®è¿‡æ»¤
        if train_start:
            df = df[df["date"] >= train_start]
        if train_end:
            df = df[df["date"] <= train_end]
        
        # ç§»é™¤æ— æ•ˆæ•°æ®
        df = df.dropna(subset=["label_score"])
        
        if len(df) < 100:
            logger.error(f"è®­ç»ƒæ•°æ®ä¸è¶³: {len(df)} è¡Œ")
            return {"status": "error", "message": "è®­ç»ƒæ•°æ®ä¸è¶³"}
        
        logger.info(f"è®­ç»ƒæ•°æ®èŒƒå›´: {df['date'].min()} ~ {df['date'].max()}")
        logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(df)}")
        
        # å‡†å¤‡ç‰¹å¾
        available_features = [c for c in self.feature_columns if c in df.columns]
        X = df[available_features].fillna(0).values
        y = df["label_score"].values
        
        # è·å–å®é™…æ”¶ç›Šï¼ˆç”¨äºå¥–æƒ©ï¼‰
        actual_returns = df["label_next_return"].fillna(0).values if "label_next_return" in df.columns else None
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (æ—¶é—´åºåˆ—åˆ’åˆ†)
        split_idx = int(len(X) * 0.8)
        X_train_raw, X_valid_raw = X[:split_idx], X[split_idx:]
        y_train, y_valid = y[:split_idx], y[split_idx:]
        
        if actual_returns is not None:
            returns_train, returns_valid = actual_returns[:split_idx], actual_returns[split_idx:]
        else:
            returns_train, returns_valid = None, None
        
        # æ ‡å‡†åŒ–
        X_train = self.scaler.fit_transform(X_train_raw)
        X_valid = self.scaler.transform(X_valid_raw)
        
        # è½¬ä¸ºTensor
        X_train_t = torch.FloatTensor(X_train).to(self.device)
        y_train_t = torch.FloatTensor(y_train).view(-1, 1).to(self.device)
        X_valid_t = torch.FloatTensor(X_valid).to(self.device)
        y_valid_t = torch.FloatTensor(y_valid).view(-1, 1).to(self.device)
        
        if returns_train is not None:
            returns_train_t = torch.FloatTensor(returns_train).view(-1, 1).to(self.device)
            returns_valid_t = torch.FloatTensor(returns_valid).view(-1, 1).to(self.device)
        else:
            returns_train_t, returns_valid_t = None, None
        
        # åˆ›å»ºDataLoader
        if returns_train_t is not None:
            train_dataset = TensorDataset(X_train_t, y_train_t, returns_train_t)
        else:
            train_dataset = TensorDataset(X_train_t, y_train_t)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        input_dim = len(available_features)
        self.model = RewardETFNet(input_dim, hidden_dims=[128, 64, 32]).to(self.device)
        
        # å¥–æƒ©æŸå¤±å‡½æ•° - è½»æƒ©ç½šé‡å¥–åŠ±
        criterion = RewardLoss(
            reward_weight=reward_weight, 
            penalty_weight=penalty_weight,
            top_k_strict=top_k_strict,
            big_gain_threshold=big_gain_threshold,
            big_gain_reward_multiplier=big_gain_reward_multiplier
        )
        optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
        
        logger.info(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")
        logger.info(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬, éªŒè¯é›†: {len(X_valid)} æ ·æœ¬")
        
        # è®­ç»ƒå¾ªç¯
        best_valid_loss = float('inf')
        best_model_state = None
        patience = 15
        patience_counter = 0
        history = {"train_loss": [], "valid_loss": [], "ndcg": []}
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            
            for batch in train_loader:
                if len(batch) == 3:
                    batch_X, batch_y, batch_returns = batch
                else:
                    batch_X, batch_y = batch
                    batch_returns = None
                
                optimizer.zero_grad()
                outputs = self.model(batch_X)
                loss = criterion(outputs, batch_y, batch_returns)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item() * batch_X.size(0)
            
            train_loss /= len(train_loader.dataset)
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            with torch.no_grad():
                valid_outputs = self.model(X_valid_t)
                valid_loss = criterion(valid_outputs, y_valid_t, returns_valid_t).item()
            
            # å­¦ä¹ ç‡è°ƒæ•´
            scheduler.step(valid_loss)
            
            # è®°å½•å†å²
            history["train_loss"].append(train_loss)
            history["valid_loss"].append(valid_loss)
            
            if (epoch + 1) % 10 == 0:
                logger.info(f"Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.4f} | Valid Loss: {valid_loss:.4f}")
            
            # æ—©åœ
            if valid_loss < best_valid_loss:
                best_valid_loss = valid_loss
                best_model_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    logger.info(f"â¹ï¸ Early stopping at epoch {epoch+1}")
                    break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if best_model_state:
            self.model.load_state_dict(best_model_state)
            self.model.to(self.device)
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        self.model.eval()
        with torch.no_grad():
            y_pred = self.model(X_valid_t).cpu().numpy().flatten()
        
        mse = mean_squared_error(y_valid, y_pred)
        
        # è®¡ç®—NDCG
        df_valid = df.iloc[split_idx:].copy()
        df_valid["pred_score"] = y_pred
        
        ndcg_scores = []
        for date in df_valid["date"].unique():
            date_df = df_valid[df_valid["date"] == date]
            if len(date_df) < 2:
                continue
            try:
                ndcg = ndcg_score(
                    date_df["label_score"].values.reshape(1, -1),
                    date_df["pred_score"].values.reshape(1, -1),
                    k=5
                )
                ndcg_scores.append(ndcg)
            except:
                pass
        
        avg_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0
        
        # ä¿å­˜æ¨¡å‹
        self.save_model()
        
        result = {
            "status": "success",
            "train_samples": len(X_train),
            "valid_samples": len(X_valid),
            "mse": mse,
            "ndcg@5": avg_ndcg,
            "best_valid_loss": best_valid_loss,
            "epochs_trained": epoch + 1,
        }
        
        logger.info("=" * 60)
        logger.info(f"âœ… å¥–æƒ©æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        logger.info(f"   MSE: {mse:.4f} | NDCG@5: {avg_ndcg:.4f}")
        logger.info("=" * 60)
        
        return result
    
    def predict(self, df: pd.DataFrame, top_k: int = 5) -> pd.DataFrame:
        """é¢„æµ‹ETFå¾—åˆ†"""
        if self.model is None:
            self.load_model()
            if self.model is None:
                logger.error("æ¨¡å‹æœªåŠ è½½")
                return pd.DataFrame()
        
        available_features = [c for c in self.feature_columns if c in df.columns]
        X = df[available_features].fillna(0).values
        
        self.model.eval()
        with torch.no_grad():
            X_scaled = self.scaler.transform(X)
            X_tensor = torch.FloatTensor(X_scaled).to(self.device)
            scores = self.model(X_tensor).cpu().numpy().flatten()
        
        result = df[["date", "etf_code", "etf_name"]].copy()
        result["pred_score"] = scores
        result = result.sort_values("pred_score", ascending=False)
        result["rank"] = range(1, len(result) + 1)
        
        return result.head(top_k)
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is not None:
            MODEL_DIR.mkdir(parents=True, exist_ok=True)
            torch.save(self.model.state_dict(), self.model_path)
            with open(self.scaler_path, "wb") as f:
                pickle.dump(self.scaler, f)
            logger.info(f"å¥–æƒ©æ¨¡å‹å·²ä¿å­˜: {self.model_path}")
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹"""
        if self.model_path.exists() and self.scaler_path.exists():
            try:
                with open(self.scaler_path, "rb") as f:
                    self.scaler = pickle.load(f)
                
                n_features = self.scaler.n_features_in_
                self.model = RewardETFNet(n_features, hidden_dims=[128, 64, 32]).to(self.device)
                self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))
                self.model.eval()
                
                logger.info(f"å¥–æƒ©æ¨¡å‹å·²åŠ è½½")
                return True
            except Exception as e:
                logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return False
        return False


class RewardRollingBacktest:
    """
    å¥–æƒ©æœºåˆ¶æ»šåŠ¨å›æµ‹å¼•æ“
    
    ç­–ç•¥ï¼š
    1. ä½¿ç”¨2022-2023å¹´æ•°æ®æŒ‰æœˆæ»šåŠ¨è®­ç»ƒ
    2. æ¯æœˆé‡æ–°è®­ç»ƒï¼Œçº³å…¥æ–°æ•°æ®
    3. 2024å…¨å¹´ä½œä¸ºå›æµ‹æœŸ
    """
    
    def __init__(self, 
                 train_window_months: int = 12,
                 retrain_interval_months: int = 1,
                 top_k: int = 5,
                 reward_weight: float = 0.5,
                 penalty_weight: float = 0.8):
        """
        Args:
            train_window_months: è®­ç»ƒçª—å£ï¼ˆæœˆï¼‰
            retrain_interval_months: é‡è®­ç»ƒé—´éš”ï¼ˆæœˆï¼‰
            top_k: é¢„æµ‹Top-K
            reward_weight: å¥–åŠ±æƒé‡
            penalty_weight: æƒ©ç½šæƒé‡
        """
        self.train_window_months = train_window_months
        self.retrain_interval_months = retrain_interval_months
        self.top_k = top_k
        self.reward_weight = reward_weight
        self.penalty_weight = penalty_weight
        
        self.model = ETFRewardModel()
        self.results = []
    
    def run_backtest(self, df: pd.DataFrame,
                     train_start: str = "2022-01-01",
                     train_end: str = "2023-12-31",
                     test_start: str = "2024-01-01",
                     test_end: str = "2024-12-31") -> Dict:
        """
        æ‰§è¡Œæ»šåŠ¨å›æµ‹
        
        Args:
            df: å®Œæ•´å†å²æ•°æ®ï¼ˆå«ç‰¹å¾å’Œæ ‡ç­¾ï¼‰
            train_start/train_end: åˆå§‹è®­ç»ƒæœŸ
            test_start/test_end: å›æµ‹æœŸ
        """
        logger.info("=" * 70)
        logger.info("ğŸš€ å¼€å§‹å¥–æƒ©æœºåˆ¶æ»šåŠ¨å›æµ‹")
        logger.info(f"   è®­ç»ƒæœŸ: {train_start} ~ {train_end}")
        logger.info(f"   å›æµ‹æœŸ: {test_start} ~ {test_end}")
        logger.info(f"   è®­ç»ƒçª—å£: {self.train_window_months}ä¸ªæœˆ")
        logger.info(f"   é‡è®­ç»ƒé—´éš”: {self.retrain_interval_months}ä¸ªæœˆ")
        logger.info("=" * 70)
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼
        df["date"] = pd.to_datetime(df["date"]).dt.strftime("%Y-%m-%d")
        df = df.sort_values(["date", "etf_code"]).reset_index(drop=True)
        
        # è·å–å›æµ‹æœŸçš„æ‰€æœ‰äº¤æ˜“æ—¥
        test_df = df[(df["date"] >= test_start) & (df["date"] <= test_end)]
        test_dates = sorted(test_df["date"].unique())
        
        if len(test_dates) == 0:
            logger.error("å›æµ‹æœŸæ— æ•°æ®!")
            return {"status": "error", "message": "å›æµ‹æœŸæ— æ•°æ®"}
        
        logger.info(f"å›æµ‹äº¤æ˜“æ—¥æ•°: {len(test_dates)}")
        
        # åˆå§‹åŒ–
        all_predictions = []
        monthly_returns = []
        current_train_end = train_end
        last_retrain_month = None
        
        # åˆå§‹è®­ç»ƒ
        logger.info("\nğŸ“š åˆå§‹æ¨¡å‹è®­ç»ƒ...")
        train_data = df[(df["date"] >= train_start) & (df["date"] <= train_end)]
        self.model.train_with_reward(
            train_data,
            epochs=100,
            reward_weight=self.reward_weight,
            penalty_weight=self.penalty_weight
        )
        
        # é€æ—¥å›æµ‹
        for i, test_date in enumerate(test_dates):
            test_month = test_date[:7]  # YYYY-MM
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒï¼ˆæ¯æœˆåˆï¼‰
            if last_retrain_month is None or test_month != last_retrain_month:
                if last_retrain_month is not None:
                    # æ›´æ–°è®­ç»ƒçª—å£
                    new_train_end = (datetime.strptime(test_date, "%Y-%m-%d") - timedelta(days=1)).strftime("%Y-%m-%d")
                    new_train_start = (datetime.strptime(new_train_end, "%Y-%m-%d") - 
                                      timedelta(days=self.train_window_months * 30)).strftime("%Y-%m-%d")
                    
                    logger.info(f"\nğŸ”„ [{test_month}] é‡æ–°è®­ç»ƒæ¨¡å‹...")
                    logger.info(f"   æ–°è®­ç»ƒæœŸ: {new_train_start} ~ {new_train_end}")
                    
                    retrain_data = df[(df["date"] >= new_train_start) & (df["date"] <= new_train_end)]
                    if len(retrain_data) > 100:
                        self.model.train_with_reward(
                            retrain_data,
                            epochs=80,
                            reward_weight=self.reward_weight,
                            penalty_weight=self.penalty_weight
                        )
                
                last_retrain_month = test_month
            
            # è·å–å½“æ—¥æ•°æ®è¿›è¡Œé¢„æµ‹
            day_data = df[df["date"] == test_date].copy()
            
            if day_data.empty:
                continue
            
            # é¢„æµ‹
            predictions = self.model.predict(day_data, top_k=self.top_k)
            
            if predictions.empty:
                continue
            
            # è·å–å®é™…æ”¶ç›Š
            for _, row in predictions.iterrows():
                etf_code = row["etf_code"]
                pred_rank = row["rank"]
                pred_score = row["pred_score"]
                
                # æŸ¥æ‰¾å®é™…æ¬¡æ—¥æ”¶ç›Š
                actual_return = day_data[day_data["etf_code"] == etf_code]["label_next_return"].values
                actual_return = actual_return[0] if len(actual_return) > 0 else 0
                
                all_predictions.append({
                    "date": test_date,
                    "etf_code": etf_code,
                    "etf_name": row["etf_name"],
                    "pred_rank": pred_rank,
                    "pred_score": pred_score,
                    "actual_return": actual_return,
                    "is_positive": actual_return > 0,
                })
            
            if (i + 1) % 20 == 0:
                logger.info(f"   å·²å®Œæˆ {i+1}/{len(test_dates)} ä¸ªäº¤æ˜“æ—¥")
        
        # ç”Ÿæˆå›æµ‹æŠ¥å‘Š
        results_df = pd.DataFrame(all_predictions)
        report = self._generate_report(results_df)
        
        # ä¿å­˜ç»“æœ
        self._save_results(results_df, report)
        
        return report
    
    def _generate_report(self, results_df: pd.DataFrame) -> Dict:
        """ç”Ÿæˆå›æµ‹æŠ¥å‘Š"""
        if results_df.empty:
            return {"status": "error", "message": "æ— é¢„æµ‹ç»“æœ"}
        
        # åˆå§‹æœ¬é‡‘
        INITIAL_CAPITAL = 10000.0
        
        # åŸºç¡€ç»Ÿè®¡
        total_predictions = len(results_df)
        total_positive = results_df["is_positive"].sum()
        hit_rate = total_positive / total_predictions if total_predictions > 0 else 0
        
        # æŒ‰æ’åç»Ÿè®¡
        top1_results = results_df[results_df["pred_rank"] == 1]
        top3_results = results_df[results_df["pred_rank"] <= 3]
        
        top1_hit_rate = top1_results["is_positive"].mean() if len(top1_results) > 0 else 0
        top3_hit_rate = top3_results["is_positive"].mean() if len(top3_results) > 0 else 0
        
        # æ”¶ç›Šç»Ÿè®¡
        avg_return = results_df["actual_return"].mean()
        
        # Top-1ç­–ç•¥æ”¶ç›Šï¼ˆæ¯å¤©ä¹°å…¥Top-1ï¼Œç”¨æœ¬é‡‘å¤åˆ©è®¡ç®—ï¼‰
        top1_daily_returns = top1_results.groupby("date")["actual_return"].mean()
        top1_capital = INITIAL_CAPITAL
        for ret in top1_daily_returns:
            top1_capital *= (1 + ret / 100)
        top1_final_capital = top1_capital
        top1_total_return = ((top1_final_capital - INITIAL_CAPITAL) / INITIAL_CAPITAL) * 100
        top1_sharpe = (top1_daily_returns.mean() / top1_daily_returns.std() * np.sqrt(252)) if top1_daily_returns.std() > 0 else 0
        
        # Top-5ç­‰æƒç­–ç•¥ï¼ˆç”¨æœ¬é‡‘å¤åˆ©è®¡ç®—ï¼‰
        top5_daily_returns = results_df.groupby("date")["actual_return"].mean()
        top5_capital = INITIAL_CAPITAL
        for ret in top5_daily_returns:
            top5_capital *= (1 + ret / 100)
        top5_final_capital = top5_capital
        top5_total_return = ((top5_final_capital - INITIAL_CAPITAL) / INITIAL_CAPITAL) * 100
        top5_sharpe = (top5_daily_returns.mean() / top5_daily_returns.std() * np.sqrt(252)) if top5_daily_returns.std() > 0 else 0
        
        # æœ€å¤§å›æ’¤
        cumulative = (1 + top5_daily_returns / 100).cumprod()
        rolling_max = cumulative.expanding().max()
        drawdown = (cumulative - rolling_max) / rolling_max * 100
        max_drawdown = drawdown.min()
        
        # èƒœç‡
        win_days = (top5_daily_returns > 0).sum()
        total_days = len(top5_daily_returns)
        win_rate = win_days / total_days if total_days > 0 else 0
        
        # æœˆåº¦ç»Ÿè®¡
        results_df["month"] = results_df["date"].str[:7]
        monthly_stats = results_df.groupby("month").agg({
            "actual_return": "mean",
            "is_positive": "mean"
        }).rename(columns={"actual_return": "avg_return", "is_positive": "hit_rate"})
        
        report = {
            "status": "success",
            "period": f"{results_df['date'].min()} ~ {results_df['date'].max()}",
            "total_days": total_days,
            "total_predictions": total_predictions,
            
            # æœ¬é‡‘ä¿¡æ¯
            "initial_capital": INITIAL_CAPITAL,
            "top1_final_capital": top1_final_capital,
            "top5_final_capital": top5_final_capital,
            
            # å‘½ä¸­ç‡
            "overall_hit_rate": hit_rate,
            "top1_hit_rate": top1_hit_rate,
            "top3_hit_rate": top3_hit_rate,
            
            # æ”¶ç›Šï¼ˆå¹´åŒ–ï¼‰
            "avg_daily_return": avg_return,
            "top1_total_return": top1_total_return,
            "top5_total_return": top5_total_return,
            
            # é£é™©æŒ‡æ ‡
            "top1_sharpe": top1_sharpe,
            "top5_sharpe": top5_sharpe,
            "max_drawdown": max_drawdown,
            "win_rate": win_rate,
            
            # æœˆåº¦æ•°æ®
            "monthly_stats": monthly_stats.to_dict(),
        }
        
        return report
    
    def _save_results(self, results_df: pd.DataFrame, report: Dict):
        """ä¿å­˜å›æµ‹ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†é¢„æµ‹
        detail_path = BACKTEST_RESULTS_DIR / f"reward_backtest_detail_{timestamp}.csv"
        results_df.to_csv(detail_path, index=False, encoding="utf-8-sig")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = BACKTEST_RESULTS_DIR / f"reward_backtest_report_{timestamp}.md"
        
        report_content = f"""# ğŸ¯ å¥–æƒ©æœºåˆ¶æ·±åº¦å­¦ä¹ å›æµ‹æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

## ğŸ“Š å›æµ‹æ¦‚è§ˆ

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å›æµ‹æœŸé—´ | {report.get('period', 'N/A')} |
| äº¤æ˜“å¤©æ•° | {report.get('total_days', 0)} |
| æ€»é¢„æµ‹æ¬¡æ•° | {report.get('total_predictions', 0)} |
| åˆå§‹æœ¬é‡‘ | Â¥{report.get('initial_capital', 10000):.2f} |

---

## ğŸ¯ å‘½ä¸­ç‡ç»Ÿè®¡

| æ’å | å‘½ä¸­ç‡ |
|------|--------|
| Top-1 | {report.get('top1_hit_rate', 0):.2%} |
| Top-3 | {report.get('top3_hit_rate', 0):.2%} |
| æ•´ä½“ | {report.get('overall_hit_rate', 0):.2%} |

---

## ğŸ’° æ”¶ç›Šç»Ÿè®¡ï¼ˆæœ¬é‡‘ Â¥10,000ï¼‰

| ç­–ç•¥ | æœ€ç»ˆèµ„é‡‘ | å¹´åŒ–æ”¶ç›Šç‡ | å¤æ™®æ¯”ç‡ |
|------|----------|------------|----------|
| Top-1ç­–ç•¥ | Â¥{report.get('top1_final_capital', 10000):.2f} | {report.get('top1_total_return', 0):.2f}% | {report.get('top1_sharpe', 0):.2f} |
| Top-5ç­‰æƒ | Â¥{report.get('top5_final_capital', 10000):.2f} | {report.get('top5_total_return', 0):.2f}% | {report.get('top5_sharpe', 0):.2f} |

---

## ğŸ“‰ é£é™©æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| æœ€å¤§å›æ’¤ | {report.get('max_drawdown', 0):.2f}% |
| èƒœç‡ | {report.get('win_rate', 0):.2%} |
| å¹³å‡æ—¥æ”¶ç›Š | {report.get('avg_daily_return', 0):.4f}% |

---

## ğŸ“… æœˆåº¦è¡¨ç°

"""
        
        monthly_stats = report.get('monthly_stats', {})
        if monthly_stats:
            report_content += "| æœˆä»½ | å¹³å‡æ”¶ç›Š | å‘½ä¸­ç‡ |\n|------|----------|--------|\n"
            avg_returns = monthly_stats.get('avg_return', {})
            hit_rates = monthly_stats.get('hit_rate', {})
            for month in sorted(avg_returns.keys()):
                ret = avg_returns.get(month, 0)
                hit = hit_rates.get(month, 0)
                report_content += f"| {month} | {ret:.2f}% | {hit:.2%} |\n"
        
        report_content += """
---

## âš ï¸ å…è´£å£°æ˜

æœ¬å›æµ‹ç»“æœä»…ä¾›ç ”ç©¶å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚å†å²è¡¨ç°ä¸ä»£è¡¨æœªæ¥æ”¶ç›Šã€‚

*æŠ¥å‘Šç”±å¥–æƒ©æœºåˆ¶æ·±åº¦å­¦ä¹ ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“„ å›æµ‹è¯¦æƒ…å·²ä¿å­˜: {detail_path}")
        logger.info(f"ğŸ“„ å›æµ‹æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def print_report(self, report: Dict):
        """æ‰“å°å›æµ‹æŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ¯ å¥–æƒ©æœºåˆ¶æ·±åº¦å­¦ä¹ å›æµ‹æŠ¥å‘Š")
        print("=" * 70)
        
        print(f"\nğŸ“Š å›æµ‹æœŸé—´: {report.get('period', 'N/A')}")
        print(f"   äº¤æ˜“å¤©æ•°: {report.get('total_days', 0)}")
        print(f"   æ€»é¢„æµ‹æ•°: {report.get('total_predictions', 0)}")
        print(f"   åˆå§‹æœ¬é‡‘: Â¥{report.get('initial_capital', 10000):.2f}")
        
        print(f"\nğŸ¯ å‘½ä¸­ç‡:")
        print(f"   Top-1 å‘½ä¸­ç‡: {report.get('top1_hit_rate', 0):.2%}")
        print(f"   Top-3 å‘½ä¸­ç‡: {report.get('top3_hit_rate', 0):.2%}")
        print(f"   æ•´ä½“å‘½ä¸­ç‡:   {report.get('overall_hit_rate', 0):.2%}")
        
        print(f"\nğŸ’° æ”¶ç›Šç»Ÿè®¡ï¼ˆæœ¬é‡‘ Â¥10,000ï¼‰:")
        print(f"   Top-1ç­–ç•¥æœ€ç»ˆèµ„é‡‘: Â¥{report.get('top1_final_capital', 10000):.2f}")
        print(f"   Top-1ç­–ç•¥å¹´åŒ–æ”¶ç›Š: {report.get('top1_total_return', 0):.2f}%")
        print(f"   Top-5ç­‰æƒæœ€ç»ˆèµ„é‡‘: Â¥{report.get('top5_final_capital', 10000):.2f}")
        print(f"   Top-5ç­‰æƒå¹´åŒ–æ”¶ç›Š: {report.get('top5_total_return', 0):.2f}%")
        print(f"   å¹³å‡æ—¥æ”¶ç›Š: {report.get('avg_daily_return', 0):.4f}%")
        
        print(f"\nğŸ“ˆ é£é™©æŒ‡æ ‡:")
        print(f"   Top-1 å¤æ™®æ¯”ç‡: {report.get('top1_sharpe', 0):.2f}")
        print(f"   Top-5 å¤æ™®æ¯”ç‡: {report.get('top5_sharpe', 0):.2f}")
        print(f"   æœ€å¤§å›æ’¤: {report.get('max_drawdown', 0):.2f}%")
        print(f"   èƒœç‡: {report.get('win_rate', 0):.2%}")
        
        print("\n" + "=" * 70)


# å¯¼å…¥å¸¸é‡
BACKTEST_RESULTS_DIR = Path(__file__).parent.parent / "data" / "backtest_results"
BACKTEST_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
